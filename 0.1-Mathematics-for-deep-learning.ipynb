{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ygUAc7ZsdHiKrTV7C3Z5wIg3u2EhV_Ht","timestamp":1663170046685}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0PqWHz--qlqN"},"source":["#Mathematics for Deep Learning\n","In this lab, we will review some basic mathematical concepts that will appear during this course and their usage in Python. We will have to use various Python libraries, such as [Numpy](https://numpy.org/) and [Scipy](https://www.scipy.org/)."]},{"cell_type":"markdown","metadata":{"id":"1AGus6tjT21o"},"source":["##Linear Algebra\n","Machine learning in general, and neural networks in particular, make an intensive use of linear algebra concepts. In Python, we use Numpy to work with linear algebra. Let's see some examples."]},{"cell_type":"markdown","metadata":{"id":"99RztpJuUdMS"},"source":["###Scalars, Vectors, Matrices and Tensors\n","The study of linear algebra involves several types of mathematical objects:\n","\n","\n","*   **Scalars:** A scalar is just a single number, in contrast to most of the other objects studied in linear algebra, which are usually arrays of multiple numbers.\n","*   **Vectors:** A vector is an array of numbers. The numbers are arranged in order. We can identify each individual number by its index in that ordering. We also need to say what kind of numbers are stored in the vector. If each element is in $\\mathbb{R}$, and the vector has $n$ elements, then the vector lies in the set formed by taking the Cartesian product of $\\mathbb{R}$ $n$ times, denoted as $\\mathbb{R}^n$. We can think of vectors as identifying points in space, with each element giving the coordinate along a diﬀerent axis.\n","*   **Matrices:** A matrix is a 2-D array of numbers, so each element is identiﬁed by two indices instead of just one. If a real-valued matrix $A$ has a height of $m$ and a width of $n$, then we say that $A ∈ \\mathbb{R}^{m \\times n}$.\n","*   **Tensors:** In some cases we will need an array with more than two axes. In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor. For example, a tensor $A \\in \\mathbb{R}^{m \\times n \\times o}$ is a 3-D tensor, where the first axis has $m$ elements, the second axis $n$ elements and the third axis $o$. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"swTLr7kSXyWn"},"source":["**EXERCISE:** Using Numpy, declare the following variables:\n","\n","\n","*   `v`: a vector of size 5 with all the elements initialized with the scalar 2.\n","*   `M`: an identity matrix of size $5 \\times 5$.\n","*   `T`: a tensor of size $3 \\times 3 \\times 3$ initialized with the scalar 0.\n","\n","*Hint:* have a look at numpy array initialization functions in the web. \n","\n"]},{"cell_type":"code","metadata":{"id":"Qfpxjji7qeIz"},"source":["import numpy as np\n","\n","### WRITE YOUR CODE HERE ### (≈ 3 lines)\n","\n","########################\n","\n","print(f'v:\\n {v}')\n","print('-----------------')\n","print(f'M:\\n {M}')\n","print('-----------------')\n","print(f'T:\\n {T}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OLVqaZk5Zz8H"},"source":["One important operation on matrices is the **transpose**. The transpose of a matrix is the mirror image of the matrix across a diagonal line, called the **main diagonal**, running down and to the right, starting from its upper left corner. We denote the transpose of amatrix $A$ as $A^T$, and it is deﬁned such that: $(A^T)_{i, j} = A_{j, i}$."]},{"cell_type":"markdown","metadata":{"id":"pC6oemFFaTVV"},"source":["**EXERCISE**: Obtain the transpose matrix of `A` using Numpy and store it in the new variable `A_T`."]},{"cell_type":"code","metadata":{"id":"s8txbfqZabco"},"source":["A = np.array([[1, 2], [3, 4], [5, 6]])\n","print(f'A:\\n {A}')\n","print(f'A shape: {A.shape}')\n","\n","### WRITE YOUR CODE HERE ### (≈ 1 line)\n","\n","########################\n","\n","print(f'Transpose of A:\\n {A_T}')\n","print(f'A_T shape: {A_T.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YJx5xvJGdrcp"},"source":["###Multiplying Matrices and Vectors\n","One of the most important operations involving matrices is multiplication of two matrices. The matrix product of matrices $A$ and $B$ is a third matrix $C$. In order for this product to be deﬁned, $A$ must have the same number of columns as $B$ has rows. If $A$ is of shape $m \\times n$ and $B$ is of shape $n \\times p$, then $C$ is of shape $m \\times p$. We can write the matrix product just by placing two or more matrices together, for example, $C= AB$. The product operation is defined by\n","$$ C_{i,j} = \\sum_k A_{i,k} B_{k,j}$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Tb3QdY2Sefqu"},"source":["**EXERCISE:** Multiply the matrices `A` and `B` using the suitable Numpy function and store the result in `C`."]},{"cell_type":"code","metadata":{"id":"ytAWrv6EezFD"},"source":["A = np.array([[1, 2], [3, 4], [5, 6]])\n","B = np.array([[1, 2, 3], [4, 5, 6]])\n","print(f'A shape: {A.shape}')\n","print(f'B shape: {B.shape}')\n","\n","### WRITE YOUR CODE HERE ### (≈ 1 line)\n","\n","########################\n","\n","print(f'C: \\n {C}')\n","print(f'Shape of C: {C.shape}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OT9Q3mvIf-in"},"source":["**EXERCISE:** Multiply the matrices `B` and `A` and store the results in `C`."]},{"cell_type":"code","metadata":{"id":"3ZQBJW9bgGoF"},"source":["### WRITE YOUR CODE HERE ### (≈ 1 line)\n","\n","########################\n","\n","print(f'C: \\n {C}')\n","print(f'Shape of C: {C.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gCHwQ2vzgW9G"},"source":["As can be seen, the order of the matrices in matrix multiplication is very important. "]},{"cell_type":"markdown","metadata":{"id":"vvk5CH6kgfE3"},"source":["Note that the standard product of two matrices is not just a matrix containing the product of the individual elements. Such an operation exists and is called the **element-wise product**, or **Hadamard product**, and is denoted as $A \\odot B$."]},{"cell_type":"markdown","metadata":{"id":"Ee0uvND2hkr1"},"source":["**EXERCISE:** multiply element-wise matrices `A` and `B` and store the result in `C`."]},{"cell_type":"code","metadata":{"id":"-zCr7y39h38O"},"source":["A = np.array([[1, 2, 3], [4, 5, 6]])\n","B = np.array([[7, 8, 9], [10, 11, 12]])\n","\n","print(f'A shape: {A.shape}')\n","print(f'B shape: {B.shape}')\n","\n","### WRITE YOUR CODE HERE ### (≈ 1 line)\n","\n","########################\n","\n","print(f'C: \\n {C}')\n","print(f'C shape: {C.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ncm6IPFYtEJl"},"source":["###Identity and Inverse Matrices\n","To describe matrix inversion, we ﬁrst need to deﬁne the concept of an **identity matrix**. An identity matrix is a matrix that does not change any vector when we multiply that vector by that matrix. We denote the identity matrix that preserves $n$-dimensional vectors as $I_n$. Formally, $I_n \\in \\mathbb{R}^{n \\times n}$, and \n","$$ \\forall x \\in \\mathbb{R}^n , I_n x = x $$\n","where, $x \\in \\mathbb{R}^n$.\n","\n","The matrix inverse of $A$ is denoted as $A^{−1}$, and it is deﬁned as the matrix such that\n","$$ A^{-1} A = I_n$$"]},{"cell_type":"markdown","metadata":{"id":"xk4FT6Wuuzjl"},"source":["**EXERCISE:** obtain the inverse matrix of `A` and store it in `A_inv`. Check that multiplying `A_inv` and `A` you obtain the identity matrix, storing the multiplication in `C`.  "]},{"cell_type":"code","metadata":{"id":"OUV1plSLv-X8"},"source":["A = np.array([[1, 2, 3], [4, 5, 6], [7, 7, 9]])\n","print(f'A shape: {A.shape}')\n","\n","### WRITE YOUR CODE HERE ### (≈ 2 line)\n","\n","########################\n","\n","print(f'Inverse of A:\\n {A}')\n","print(f'A_inv shape: {A.shape}')\n","print('---------------------')\n","print(f'C: \\n {C}')\n","print(f'C shape: {C.shape}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Enl5J9Py9cl"},"source":["`C` is not actualy the expected identity matrix, but that is due to two reasons:\n","\n","\n","1.   In general, the inverse of a matrix cannot be calculated in closed form, so libraries such as Numpy use numerical approximations.\n","2.   The multiplication of two matrices depends also on floating arithmetics, which is not 100% precise.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qRakIYJEBXRB"},"source":["###Norms\n","Sometimes we need to measure the size of a vector. In machine learning, we usually measure the size of vectors using a function called a norm. Formally, the $L^p$ norm is given by:\n","$$ ||x||_p = \\left(\\sum_i |x_i |^p \\right)^{1/p} $$\n","for $p \\in \\mathbb{R}$, $p \\geq 1$.\n","\n","Norms, including the $L^p$ norm, are functions mapping vectors to non-negative values. On an intuitive level, the norm of a vector $x$ measures the distance from the origin to the point $x$. More rigorously, a norm is any function $f$ that satisfies the following properties:\n","\n","\n","*   $f(x) = 0 \\rightarrow x = 0$\n","*   $f(x, y) \\leq f(x) + f(y)$ (the **triangle inequality**)\n","*   $\\forall \\alpha \\in \\mathbb{R}, f(\\alpha x) = |\\alpha| f(x)$\n","\n","The $L^2$ norm, with $p = 2$, is known as the **Euclidean norm**, which is simply the Euclidean distance from the origin to the point identiﬁed by $x$. The $L^2$ norm is used so frequently in machine learning that it is often denoted simply as $||x||$, with the subscript 2 omitted. It is also common to measure the size of a vector using the squared $L^2$ norm, which can be calculated simply as $x^T x$.\n","\n","The squared $L^2$ norm is more convenient to work with mathematically and computationally than the $L^2$ norm itself. For example, each derivative of the squared $L^2$ norm with respect to each element of $x$ depends only on the corresponding element of $x$, while all the derivatives of the $L^2$ norm depend on the entire vector. In many contexts, the squared $L^2$ norm may be undesirable because it increases very slowly near the origin. In several machine learning applications, it is important to discriminate between elements that are exactly zero and elements that are small but nonzero. In these cases, we turn to a function that grows at the same rate in all locations, but that retains mathematical simplicity: the $L^1$ norm. The $L^1$ norm may be simpliﬁed to:\n","$$ ||x||_1 = \\sum_i |x_i| $$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"px66a1NlFBO0"},"source":["**EXERCISE:** Complete the functions `l1_norm()` and `l2_norm()` implementing manually the definitions given above. Afterwards, use also Numpy functions to calculate them, storing the results on variables `l1` and `l2`. If you do everything well, you should obtain the same norm values."]},{"cell_type":"code","metadata":{"id":"Dr6xF50xFE_9"},"source":["v = np.array([1, -2 , 3])\n","\n","def l1_norm(x):\n","  ### WRITE YOUR CODE HERE ### (≈ 1 line)\n","  \n","  ########################\n","\n","def l2_norm(x):\n","  ### WRITE YOUR CODE HERE ### (≈ 1 line)\n","  \n","  ########################\n","\n","manual_l1 = l1_norm(v)\n","manual_l2 = l2_norm(v)\n","\n","# Calculate L1 and L2 using Numpy\n","### WRITE YOUR CODE HERE ### (≈ 2 lines)\n","\n","########################\n","\n","print(f'L1 norm with numpy: {l1}; manual L1 norm: {manual_l1}')\n","print(f'L2 norm with numpy: {l2}; manual L2 norm: {manual_l2}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SAv6XP_fUx7-"},"source":["###An Important Note on Rank 1 Arrays\n","Numpy rank 1 arrays are a typical source of bugs and problems. Our advise is to avoid their usage, whenever possible. We will try to explain why with some examples. Let's start with a vector of 5 random elements."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2OfgIMpTjtw","executionInfo":{"status":"ok","timestamp":1627042802301,"user_tz":-120,"elapsed":808,"user":{"displayName":"Gorka Azkune","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrPiADPTrMsB3LH4hhvSseYE2hlbYrazfmPckY-g=s64","userId":"10951389475951625125"}},"outputId":"8bd336c2-6990-43d4-c98f-fea6c185f17f"},"source":["import numpy as np\n","\n","a = np.random.rand(5)\n","print(a)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.04197713 0.64561182 0.51748754 0.55950378 0.79908274]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rmDkfi83TvcW"},"source":["It seems our array `a` has no problem, but here is a question: is this a row vector or a column vector? It turns out it is neither. Look at its shape:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7eHVsWRlT6yg","executionInfo":{"status":"ok","timestamp":1627042858929,"user_tz":-120,"elapsed":408,"user":{"displayName":"Gorka Azkune","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrPiADPTrMsB3LH4hhvSseYE2hlbYrazfmPckY-g=s64","userId":"10951389475951625125"}},"outputId":"2b10f373-5384-4021-dbed-cfd7c8d1aaf6"},"source":["print(a.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(5,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gu-TSc9qT_iC"},"source":["This is a Rank 1 array. As it is neither a row nor a column vector, it has a funny behavior. For example, what would you expect to be $a^T$, i.e. the transpose of $a$?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1bPdf7MUTUX","executionInfo":{"status":"ok","timestamp":1627042959430,"user_tz":-120,"elapsed":231,"user":{"displayName":"Gorka Azkune","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrPiADPTrMsB3LH4hhvSseYE2hlbYrazfmPckY-g=s64","userId":"10951389475951625125"}},"outputId":"24b887a1-fe48-491d-f0a4-49adfbbedee2"},"source":["print(a.T)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.04197713 0.64561182 0.51748754 0.55950378 0.79908274]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ye7cZibfUWY0"},"source":["It's the same array! This is quite dangerous, since we are used to see a column vector as the transpose of a row vector, and vice versa. However, a rank 1 array is the tranpose of itself. There is no equivalent vector or mathematical concept in algebra.\n","\n","Another example: what would you expect if you multiply `a` and `a.T`? A matrix? Another vector?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3Kw4-DLU-ZS","executionInfo":{"status":"ok","timestamp":1627043141056,"user_tz":-120,"elapsed":256,"user":{"displayName":"Gorka Azkune","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrPiADPTrMsB3LH4hhvSseYE2hlbYrazfmPckY-g=s64","userId":"10951389475951625125"}},"outputId":"808e82f0-a7e5-4e4f-8da5-a654aba13138"},"source":["print(np.dot(a, a.T))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.6379477646643614\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N91c_WENVCHe"},"source":["It's a number! Given those unintuitive behaviors, we recommend to avoid the usage of such rank 1 arrays. Instead, if we want to represent a column vector, we would type:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0CkVqO2VVYv","executionInfo":{"status":"ok","timestamp":1627043261061,"user_tz":-120,"elapsed":222,"user":{"displayName":"Gorka Azkune","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrPiADPTrMsB3LH4hhvSseYE2hlbYrazfmPckY-g=s64","userId":"10951389475951625125"}},"outputId":"f363bc04-fdcf-4fe4-9ff3-04b611087ac9"},"source":["a = np.random.rand(5, 1)\n","print(a)\n","print(f'Shape of a: {a.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.34612269]\n"," [0.38441883]\n"," [0.41809274]\n"," [0.5737006 ]\n"," [0.22967799]]\n","Shape of a: (5, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EnoEqmEmVj6G"},"source":["Now the transpose of `a` will be a row vector, just as expected:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RiJkB450VoDV","executionInfo":{"status":"ok","timestamp":1627043325586,"user_tz":-120,"elapsed":247,"user":{"displayName":"Gorka Azkune","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrPiADPTrMsB3LH4hhvSseYE2hlbYrazfmPckY-g=s64","userId":"10951389475951625125"}},"outputId":"30bae0d9-8cc7-414e-b518-6a06a04c7756"},"source":["print(a.T)\n","print(f'Shape of a transpose: {a.T.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.34612269 0.38441883 0.41809274 0.5737006  0.22967799]]\n","Shape of a transpose: (1, 5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m-iB3981Vzyl"},"source":["Notice that when we print `a` tranpose, in this case, we have two brackets in the beginning and the end. Rank 1 arrays only have one bracket.\n","\n","If we print now the product of `a` and `a.T` we get the outer product of two vectors, which is a matrix:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFztDgKYWNsR","executionInfo":{"status":"ok","timestamp":1627043488976,"user_tz":-120,"elapsed":277,"user":{"displayName":"Gorka Azkune","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrPiADPTrMsB3LH4hhvSseYE2hlbYrazfmPckY-g=s64","userId":"10951389475951625125"}},"outputId":"25b75f42-0511-4c16-f4d7-e6ec1e48c144"},"source":["b = np.dot(a, a.T)\n","print(b)\n","print(f'Shape of b: {b.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.11980092 0.13305608 0.14471139 0.1985708  0.07949677]\n"," [0.13305608 0.14777784 0.16072272 0.22054132 0.08829255]\n"," [0.14471139 0.16072272 0.17480154 0.23986006 0.0960267 ]\n"," [0.1985708  0.22054132 0.23986006 0.32913238 0.1317664 ]\n"," [0.07949677 0.08829255 0.0960267  0.1317664  0.05275198]]\n","Shape of b: (5, 5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q_eqMbdGWnQ-"},"source":["It is quite common to see rank 1 arrays. For example, when loading many datasets, the labels are usually loaded as rank 1 arrays. Many times, it will be convenient to convert those rank 1 arrays to either row or column vectors. You can use the `reshape` function for that:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4UfVUDMQW8AZ","executionInfo":{"status":"ok","timestamp":1627043696881,"user_tz":-120,"elapsed":248,"user":{"displayName":"Gorka Azkune","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrPiADPTrMsB3LH4hhvSseYE2hlbYrazfmPckY-g=s64","userId":"10951389475951625125"}},"outputId":"934f23ef-6374-4525-f668-e31f0a6f7016"},"source":["a = np.random.randn(5)\n","print(f'Shape of a: {a.shape}')\n","a = a.reshape(5, 1)\n","print(f'Shape of a after reshaping: {a.shape}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of a: (5,)\n","Shape of a after reshaping: (5, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MqZXKDfUfEWg"},"source":["##Probability and Information Theory\n","Probability theory is a mathematical framework for representing uncertain statements. It provides a means of quantifying uncertainty as well as axioms for deriving new uncertain statements. In artiﬁcial intelligence applications, we use probability theory in two major ways: \n","\n","1.   The laws of probability tell us how AI systems should reason, so we design our algorithms to compute or approximate various expressions derived using probability theory.\n","2.   We can use probability and statistics to theoretically analyze the behavior of proposed AI systems.\n","\n","While probability theory allows us to make uncertain statements and to reason in the presence of uncertainty, information theory enables us to quantify the amount of uncertainty in a probability distribution."]},{"cell_type":"markdown","metadata":{"id":"-Apa9868h62-"},"source":["###Probability Theory\n","There are many concepts from probability theory which you should be familiar with, such as random variables, probability distributions, marginal and conditional probabilities, independence and conditional independece or expectation, variance and covariance. Given the practical nature of this lab, though, we will not review all of those concepts. We refer you to the excellent [Deep Learning Book's chapter 3](https://www.deeplearningbook.org/contents/prob.html) for further reading. "]},{"cell_type":"markdown","metadata":{"id":"ybuhXwTTpNHa"},"source":["Let's simulate a dice with six faces and uniform probabilities for each of the faces. We will do so using a uniform probability distribution and sampling from that distribution. When working with probability distributions and sampling functions, it is important to fix a `seed` value to ensure the reproducibility of experiments. Now we will simulate 100 tosses with a dice to review some interesting and useful concepts of discrete variables and probability mass functions."]},{"cell_type":"markdown","metadata":{"id":"asAutkkdt330"},"source":["**EXERCISE:** Using Numpy, simulate 100 tosses of a fair dice, i.e. sample 100 values between 1 and 6 from a uniform probability distribution and store the result in the variable `tosses`. Afterwards, calculate the average and store it in the variable `avg`."]},{"cell_type":"code","metadata":{"id":"JUqOkqypp-PN"},"source":["# Set the random seed\n","np.random.seed(42)\n","\n","### WRITE YOUR CODE HERE ### (≈ 2 lines)\n","\n","########################\n","\n","print(f'Shape of tosses: {tosses.shape}')\n","print(f'Average value of the tosses: {avg}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77PC4AMCu0Sp"},"source":["Remember that the expected value of a random variable which follows a discrete uniform distribution is $E(X) = \\frac{N+1}{2}$. In our case, $N=6$, thus $E(X) = \\frac{6+1}{2}=3.5$. We get an average value close to 3.5, but not quite there. What happens here? Let's draw a histogram to better understand the situation:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"9iRLu6SAqWZW","executionInfo":{"status":"ok","timestamp":1623073176723,"user_tz":-120,"elapsed":241,"user":{"displayName":"Gorka Azkune","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrPiADPTrMsB3LH4hhvSseYE2hlbYrazfmPckY-g=s64","userId":"10951389475951625125"}},"outputId":"342fefe5-d6b1-4a94-cdb4-08c1ce98ce53"},"source":["import matplotlib.pyplot as plt\n","\n","values, counts = np.unique(tosses, return_counts=True)\n","\n","plt.vlines(values, 0, counts, color='blue', lw=4)\n","plt.xlabel('Dice values')\n","plt.ylabel('Count')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1 2 3 4 5 6]\n","[22 10 18 19 14 17]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"A_2-cHSXs5RW"},"source":["Given that the probability of obtaining any value in a dice is $1/6$, we would expect ~$16.67$ examples for each value. But as we are using only 100 samples, we observe high variability. The **law of large numbers** tells us that as the number of tosses grows this estimate will draw closer and closer to the true underlying probability. "]},{"cell_type":"markdown","metadata":{"id":"rK4WOAqFvbHP"},"source":["**EXERCISE:** Using Numpy, simulate 10000 tosses of a fair dice (store the results in the variable `tosses`). Afterwards, calculate the average and store it in the variable `avg`."]},{"cell_type":"code","metadata":{"id":"SwB0K6uwtSS9"},"source":["# Set the random seed\n","np.random.seed(42)\n","\n","### WRITE YOUR CODE HERE ### (≈ 2 lines)\n","\n","########################\n","\n","print(f'Average value of tosses: {avg}')\n","\n","values, counts = np.unique(tosses, return_counts=True)\n","\n","plt.vlines(values, 0, counts, color='blue', lw=4)\n","plt.xlabel('Dice values')\n","plt.ylabel('Count')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbZcrEyXv4Y1"},"source":["Well, this is much better. As the number of samples increases, the average value of the random sample gets closer to the **expected value** of the underlying probability distribution.  "]},{"cell_type":"markdown","metadata":{"id":"2-N7MpeHwXs-"},"source":["We can do the same things with **continuous random variables** and **probability density functions** (PDF). The most common PDF is the Normal distribution, defined as:\n","$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$$\n","where $\\sigma$ is the standard deviation and $\\mu$ is the expected value."]},{"cell_type":"markdown","metadata":{"id":"fychX2mEx4qv"},"source":["**EXERCISE:** Sample 1000 values from a Normal distribution with $\\mu=50$ and $\\sigma=5$ and store the result in the variable `samples`. Calculate the sample average and standard deviation and store their values in the variables `avg` and `std` respectively. "]},{"cell_type":"code","metadata":{"id":"Js-tmzQTwp4V"},"source":["# Set the random seed\n","np.random.seed(42)\n","\n","### WRITE YOUR CODE HERE ### (≈ 3 lines)\n","\n","########################\n","\n","print(f'Average of samples: {avg}')\n","print(f'Standard deviation of samples: {std}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cr9F3qMUygoD"},"source":["Let's visualize this sample in a histogram. Additionally we will also plot the underlying Normal distribution, to compare both. For that purpose, we will use the Scipy library."]},{"cell_type":"code","metadata":{"id":"1jg7dJcHyj6P"},"source":["from scipy.stats import norm\n","\n","dist = norm(50, 5)\n","xplot = np.linspace(30, 70, 1000)\n","yplot = dist.pdf(xplot)\n","yplot.shape\n","\n","plt.plot(xplot, yplot, color='red')\n","\n","plt.hist(samples, bins=10, density=True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WMADFsvE0nI-"},"source":["As can be seen, our histogram follows a Normal distribution reasonably well."]},{"cell_type":"markdown","metadata":{"id":"TV8PSk3k1EM4"},"source":["###Information Theory\n","Information theory is a branch of applied mathematics that revolves around quantifying how much information is present in a signal. It was originally invented to study sending messages from discrete alphabets over a noisy channel, such as communication via radio transmission. In this context, information theory tells how to design optimal codes and calculate the expected length of messages sampled from speciﬁc probability distributions using various encoding schemes. In the context of machine learning, we can also apply information theory to continuous variables where some of these message length interpretations do not apply. This ﬁeld is fundamental to many areas of electrical engineering and computer science. During this course, we mostly use a few key ideas from information theory to characterize probability distributions or to quantify similarity between probability distributions.\n","\n","The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying “the sun rose this morning” is so uninformative as to be unnecessary to send, but a message saying “there was a solar eclipse this morning” is very informative.\n","\n","We would like to quantify information in a way that formalizes this intuition.\n","\n","\n","*   Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever.\n","*   Less likely events should have higher information content.\n","*   Independent events should have additive information. For example, ﬁnding out that a tossed coin has come up as heads twice should convey twice as much information as ﬁnding out that a tossed coin has come up as heads once.\n","\n","To satisfy all three of these properties, we deﬁne the **self-information** of an event $X = x$ to be\n","$$ I(x) = -\\log P(x) $$\n","\n","We can use natural logarithms ($\\log_e$) or base 2 logarithms ($\\log_2$), depending on our purposes. \n"]},{"cell_type":"markdown","metadata":{"id":"229AjLyn4ZFS"},"source":["Imagine we have a box filled with balls. Balls can be either black or white. In box A, 50% of balls are black. In box B, 90% of balls are black. Thus, what is the self-information of the event $C = white$ in each of the boxes, being $C$ the random variable for color? "]},{"cell_type":"markdown","metadata":{"id":"YEDkCb2S44Xq"},"source":["**EXERCISE:** given the probability distributions for box A and B, calculate the self-information of the event $C = white$ for both boxes. Use $\\log_2$ and store the values in the variables `si_a` and `si_b`."]},{"cell_type":"code","metadata":{"id":"ewtqgU775UvU"},"source":["CA = np.array([0.5, 0.5]) # Probability distribution for box A (C=black 0.5, C=white 0.5)\n","CB = np.array([0.9, 0.1]) # Probability distribution for box B (C=black 0.9, C=white 0.1)\n","\n","def self_information(p):\n","  ### WRITE YOUR CODE HERE ### (≈ 1 line)\n","  \n","  ########################\n","\n","### WRITE YOUR CODE HERE ### (≈ 2 lines)\n","\n","########################\n","\n","print(f'Self-information for C = white in box A: {si_a}')\n","print(f'Self-information for C = white in box B: {si_b}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qO5zRfOK7Mm8"},"source":["As can be seen, obtaining a white ball from box B is much more informative than obtaining it from box A. "]},{"cell_type":"markdown","metadata":{"id":"IgXGmQbt7fu_"},"source":["Self-information deals only with a single outcome. We can quantify the amount of uncertainty in an entire probability distribution using the **Shannon entropy:**\n","$$ H(x) = \\mathbb{E}_{x \\sim P} [I(x)] = - \\mathbb{E}_{x \\sim P}[\\log P(x)] = - \\sum_i P(x_i) \\log P(x_i)$$\n","\n","In other words, the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution. It gives a lower bound on the number of bits (if the logarithm is base 2, otherwise the units are different) needed on average to encode symbols drawn from a distribution $P$. Distributions that are nearly deterministic (where the outcome is nearly certain) have low entropy; distributions that are closer to uniform have high entropy."]},{"cell_type":"markdown","metadata":{"id":"3JDFUIZ88WGz"},"source":["**EXERCISE:** given the probability distributions of box A and B, implement a function to calculate their entropy. Store the results in variables `entropy_a` and `entropy_b`. Which one will be lower?"]},{"cell_type":"code","metadata":{"id":"b15C5T1p8YSB"},"source":["CA = np.array([0.5, 0.5]) # Probability distribution for box A (C=black 0.5, C=white 0.5)\n","CB = np.array([0.9, 0.1]) # Probability distribution for box B (C=black 0.9, C=white 0.1)\n","\n","def my_entropy(p):\n","  ### WRITE YOUR CODE HERE ### (≈ 1 line)\n","    \n","  ########################\n","  return out\n","\n","### WRITE YOUR CODE HERE ### (≈ 2 lines)\n","\n","########################\n","\n","print(f'Entropy of box A: {entropy_a}')\n","print(f'Entropy of box B: {entropy_b}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cxc6-kbP98m6"},"source":["**EXERCISE:** Scipy has its own function to calculate the entropy. Use it and check whether you obtain the same values as in the cell above. Do not forget to write the suitable `import` for this task."]},{"cell_type":"code","metadata":{"id":"G72Q-q9a-G-p"},"source":["CA = np.array([0.5, 0.5]) # Probability distribution for box A (C=black 0.5, C=white 0.5)\n","CB = np.array([0.9, 0.1]) # Probability distribution for box B (C=black 0.9, C=white 0.1)\n","\n","### WRITE YOUR CODE HERE ### (≈ 3 lines)\n","\n","########################\n","\n","print(f'Entropy of box A: {entropy_a}')\n","print(f'Entropy of box B: {entropy_b}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zuvGAQoX_OYP"},"source":["If we have two separate probability distributions $P(x)$ and $Q(x)$ over the same random variable $x$, we can measure how different these two distributions are using the **Kullback-Leibler (KL) divergence**:\n","$$ D_{KL}(P||Q) = \\mathbb{E}_{x \\sim P} \\left[ \\log \\frac{P(x)}{Q(x)} \\right] = \\mathbb{E}_{x \\sim P} [\\log P(x) - \\log Q(x)] = \\sum_i P(x_i) (\\log P(x_i) - \\log Q(x_i))$$\n","\n","The KL divergence has many useful properties, most notably being non-negative.The KL divergence is 0 if and only if $P$ and $Q$ are the same distribution in the case of discrete variables, or equal “almost everywhere” in the case of continuous variables. Because the KL divergence is non-negative and measures the difference between two distributions, it is often conceptualized as measuring some sort of distance between these distributions. It is not a true distance measure because it is not symmetric: $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$ for some $P$ and $Q$. This asymmetry means that there are important consequences to the choice of whether to use $D_{KL}(P||Q)$ or $D_{KL}(Q||P)$.\n"]},{"cell_type":"markdown","metadata":{"id":"xR8CE2N8AjPm"},"source":["**EXERCISE:** Implement the function `kl_divergence()` and use it to calculate the divergence between `p` and `q` and also `p` and `r`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T7ytfQNqBDC0","executionInfo":{"status":"ok","timestamp":1623079567612,"user_tz":-120,"elapsed":234,"user":{"displayName":"Gorka Azkune","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrPiADPTrMsB3LH4hhvSseYE2hlbYrazfmPckY-g=s64","userId":"10951389475951625125"}},"outputId":"ed157bc4-0434-42f6-949a-a4a7ac9ea0eb"},"source":["p = np.array([0.10, 0.40, 0.50])\n","q = np.array([0.80, 0.15, 0.05])\n","r = np.array([0.11, 0.39, 0.50])\n","\n","def kl_divergence(p, q):\n","  ### WRITE YOUR CODE HERE ### (≈ 3 lines)\n","     \n","  ########################\n","  return out\n","\n","### WRITE YOUR CODE HERE ### (≈ 2 lines)\n","\n","########################\n","\n","print(f'KL divergence for P and Q: {kl_pq}') \n","print(f'KL divergence for P and R: {kl_pr}') "],"execution_count":null,"outputs":[{"output_type":"stream","text":["KL divergence for P and Q: 1.9269790471552188\n","KL divergence for P and R: 0.0008599980350521893\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MQtd4zh_GJzY"},"source":["As expected, `p` and `r` are *closer* than `p` and `q`. You should obtain $D_{KL}(P||Q) \\approx 1.927$ and $D_{KL}(P||R) \\approx 0.00086$."]},{"cell_type":"markdown","metadata":{"id":"uc0jlnPeGkd9"},"source":["A quantity that is closely related to the KL divergence is the **cross-entropy** $H(P, Q) = H(P) + D_{KL}(P||Q)$, which is similar to the KL divergence but lacking the term on the left:\n","$$ H(P, Q) = \\mathbb{E}_{x \\sim P} \\log Q(x) $$\n","\n","Minimizing the cross-entropy with respect to $Q$ is equivalent to minimizing the KL divergence, because $Q$ does not participate in the omitted term."]}]}