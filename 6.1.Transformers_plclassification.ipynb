{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_wU0F0JYZIC8YE1nwqG13R-i1UOJk8O2","timestamp":1673190594960}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qzpmDv01kKWl"},"source":["\n","# Using Transformers for Programming Language Classification\n","\n","In this laboratory we will implement a self-attention module and use it to perform programming language classification. The input to the system is a program in some programming language, and the model has to predict the programming language used."]},{"cell_type":"code","metadata":{"id":"S_9v_JCQkR7n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673188270451,"user_tz":-60,"elapsed":21162,"user":{"displayName":"David Bernal","userId":"05881219251979022189"}},"outputId":"5e2007b9-250a-4988-867c-2c8dcc8df476"},"source":["import torch\n","import datetime\n","from torch.utils.tensorboard import SummaryWriter\n","print(torch.__version__) \n","\n","torch.manual_seed(1212) # set seed to replicate results\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # use GPU if available\n","print(device)\n","\n","! pip install tokenizers\n","from tokenizers import Tokenizer\n","\n","# Download data\n","\n","! wget -O mlnn_lab5.4.zip https://ehubox.ehu.eus/s/xFtYB7zikesHw5r/download\n","! unzip mlnn_lab5.4.zip\n","! ls\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["1.13.0+cu116\n","cuda\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tokenizers\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers\n","Successfully installed tokenizers-0.13.2\n","--2023-01-08 14:31:00--  https://ehubox.ehu.eus/s/xFtYB7zikesHw5r/download\n","Resolving ehubox.ehu.eus (ehubox.ehu.eus)... 158.227.0.95\n","Connecting to ehubox.ehu.eus (ehubox.ehu.eus)|158.227.0.95|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5249522 (5.0M) [application/zip]\n","Saving to: ‘mlnn_lab5.4.zip’\n","\n","mlnn_lab5.4.zip     100%[===================>]   5.01M   791KB/s    in 7.8s    \n","\n","2023-01-08 14:31:09 (656 KB/s) - ‘mlnn_lab5.4.zip’ saved [5249522/5249522]\n","\n","Archive:  mlnn_lab5.4.zip\n","  inflating: mlnn_lab5.4/dev.txt     \n","  inflating: mlnn_lab5.4/labels.txt  \n","  inflating: mlnn_lab5.4/test.txt    \n","  inflating: mlnn_lab5.4/tokenizer.json  \n","  inflating: mlnn_lab5.4/train.txt   \n","mlnn_lab5.4  mlnn_lab5.4.zip  sample_data\n"]}]},{"cell_type":"code","metadata":{"id":"4HbfKl8WurTR","executionInfo":{"status":"ok","timestamp":1673188294669,"user_tz":-60,"elapsed":237,"user":{"displayName":"David Bernal","userId":"05881219251979022189"}}},"source":[],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ydDAyCoukT4o"},"source":["## Setting the hyperparameters"]},{"cell_type":"code","metadata":{"id":"_NnUDhjEkUYr","executionInfo":{"status":"ok","timestamp":1673188299047,"user_tz":-60,"elapsed":235,"user":{"displayName":"David Bernal","userId":"05881219251979022189"}}},"source":["# Hyper-parameters\n","\n","LEARNING_RATE = 0.0001 # should be right\n","HIDDEN_DIM = 128 # The dimension of the hidden state of the Self attention module\n","NUM_LAYERS = 4 # number of encoder layers\n","DROPOUT_PROB = 0.1 # dropout probability\n","BATCH_SIZE = 256 # batch size\n","NUM_EPOCHS = 50 # number of epochs\n","N_LABELS = 21 # nmber of classes (see 'mlnn_lab5.4/labels.txt')\n","SEQ_LEN = 150 # sequence max length\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lGYKSZm1kX66"},"source":["## The dataset class\n","\n","As always, we use `Dataset`and the `torch.utils.data.Dataloader`, which will create the batches for us. Note that the dataset receives a tokenizer as input, which will split the input text into (sub)tokens.\n","\n","Each instance is composed by a triplet $(x^{(i)}, m^{(i)}, y^{(i)})$ where:\n","\n","- $x^{(i)}$ is a tensor with `seq_len` (sub)token ids. If the instance has less than `seq_len` tokens, the tensor is padded (and the corresponding mask is zero). The first element corresponds to the special `[CLS]`token.\n","- $m^{(i)}$ is a mask tensor of `seq_len` 0/1 values. If zero, the corresponding (sub)token is masked.\n","- $y^{(i)}$ is a 1 dimension tensor with the class label for the sequence.\n"]},{"cell_type":"code","metadata":{"id":"CPWJUWV0ka5v","executionInfo":{"status":"ok","timestamp":1673188303547,"user_tz":-60,"elapsed":218,"user":{"displayName":"David Bernal","userId":"05881219251979022189"}}},"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, fname, tokenizer):\n","        super(Dataset, self).__init__()\n","        self.data = []\n","        for i,line in enumerate(open(fname)):\n","            x, lbl = line.strip().split('\\t')\n","            x = tokenizer.encode(x)\n","            if len(x) > SEQ_LEN:\n","                continue\n","            self.data.append([x.ids, x.attention_mask, [int(lbl)]])\n","\n","    def __getitem__(self, idx):\n","        '''Select a tuple that is further passed to collate_fn'''\n","        sent = self.data[idx][0]\n","        mask = self.data[idx][1]\n","        lbl = self.data[idx][2]\n","        return torch.LongTensor(sent), torch.LongTensor(mask), torch.LongTensor(lbl)\n","\n","    def __len__(self):\n","        '''Return the length of the dataset.'''\n","        return len(self.data)\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DbGkXqBjaXM6"},"source":["# Tokenizer class\n","\n","The tokenizer converts input strings to a list of tokens, and returns the following. Look at the code below to understand how it works. **NOTE** how the tokenizer prepends the special `[CLS]` token at the beginning of the sequence."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oHCd69PDaBPd","executionInfo":{"status":"ok","timestamp":1673188894740,"user_tz":-60,"elapsed":255,"user":{"displayName":"David Bernal","userId":"05881219251979022189"}},"outputId":"f198b316-562d-4463-d29e-65f1e818b5ad"},"source":["tok = Tokenizer.from_file(\"mlnn_lab5.4/tokenizer.json\")\n","tok.enable_padding(direction='right',length=SEQ_LEN) # max sequence is 150\n","enc = tok.encode('int main(int argc, char**argv[])')\n","print(enc.ids)\n","print(enc.attention_mask)\n","print(enc.tokens)\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 1965, 2200, 12, 1965, 4434, 16, 2143, 14, 14, 3707, 63, 65, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['[CLS]', 'int', 'main', '(', 'int', 'argc', ',', 'char', '*', '*', 'argv', '[', ']', ')', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"]}]},{"cell_type":"markdown","metadata":{"id":"JDj7ujGgkgYy"},"source":["## Class to encode input embeddings\n","\n","We calculate input embeddings by summing up two embedding types (of class `torch.nn.Embedding`)\n","\n","- token embeddings (`vocab_size` $\\times$ `embedding_dim`)\n","- positional embeddings (`vocab_size` $\\times$ `embedding_dim`)"]},{"cell_type":"code","metadata":{"id":"E8CQYOC8krs3","executionInfo":{"status":"ok","timestamp":1673189103296,"user_tz":-60,"elapsed":316,"user":{"displayName":"David Bernal","userId":"05881219251979022189"}}},"source":["class InputEmbedding(torch.nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(InputEmbedding, self).__init__()\n","        self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n","        self.positional_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n","\n","    def forward(self, x):\n","        return self.embeddings(x) + self.positional_embeddings(x)\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lt7g-bCyktju"},"source":["## Self Attention module\n","\n","This module perform a self-attention step on the input tokens in\n","$x$.\n","\n","The self-attention is performed in the usual way:\n","\n","$\\begin{aligned}\n","  & Q=X\\cdot W_Q\\\\\n","  & K=X\\cdot W_K\\\\\n","  & V=X\\cdot W_V\\\\\n","  & \\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n","\\end{aligned}$\n","\n","where $d_k$ is the dimension of the hidden vectors. However, instead\n","of matrix multiplication we will use linear layers (`torch.nn.Linear`)\n","for representing all $W_Q, W_K$ and $W_V$\n","\n","## `forward` method. It receives two inputs:\n","\n","- `x` (size `batch_size` $\\times$ `seq_len` $\\times$ `hidden_dim`): subtoken embeddings.\n","- `mask` (size `batch_size` $\\times$ `seq_len`): a mask tensor where $m_{bi} == 0$ if the $i$th word of instance $b$ is masked. If so, the value of the logit has to be set to $-1e^{10}$. See the comments below.\n","\n","The output is a tensor of size (`batch_size` $\\times$ `seq_len` $\\times$ `hidden_dim`)\n","\n","**EXERCISE**: complete the `forward` function below (see the comments in the function).\n","\n"]},{"cell_type":"code","metadata":{"id":"j0MWUMR9lVXu","executionInfo":{"status":"ok","timestamp":1673189728747,"user_tz":-60,"elapsed":507,"user":{"displayName":"David Bernal","userId":"05881219251979022189"}}},"source":["class SelfAttention(torch.nn.Module):\n","    def __init__(self, hidden_dim):\n","        super(SelfAttention, self).__init__()\n","        # hidden dimension has to be multiple of number of heads\n","        self.Mq = torch.nn.Linear(hidden_dim, hidden_dim)\n","        self.Mk = torch.nn.Linear(hidden_dim, hidden_dim)\n","        self.Mv = torch.nn.Linear(hidden_dim, hidden_dim)\n","        self.scale = float(hidden_dim) ** 0.5 # torch.sqrt(torch.tensor(hidden_dim))\n","\n","    def forward(self, x, mask):\n","        # TODO: implement the forward pass of the self-attention module. Given an input\n","        # x of shape (batch_size, seq_len, input_dim), apply self-attention to obtain\n","        # a new vector y of shape (batch_size, seq_len, hidden_dim).\n","\n","        q = self.Mq(x) # q = [batch_size, seq_len, hidden_dim]\n","        k = self.Mk(x) # k = [batch_size, seq_len, hidden_dim]\n","        v = self.Mv(x) # v = [batch_size, seq_len, hidden_dim]\n","        out = None\n","\n","        ### WRITE YOUR CODE HERE ###\n","        #\n","        # 1. Obtain attention weights\n","        # 2. Mask attention weights, and apply softmax\n","        # 3. Obtain out vector by applying attention on v\n","        #\n","        # Useful functions:\n","        #\n","        # - torch.matmul(x,y): matrix multiplication\n","        # - torch.softmax(x, dim): apply softmax in the required dimension.\n","        #\n","        # Useful Tensor methods:\n","        #\n","        # - permute(shape): permute tensor to the new shape. Shape parameter\n","        #   has to be complatible with tensor shape.\n","        # - masked_fill_(condition, value): set value to elements that fullfil\n","        #   contition. Typical usage:\n","        #\n","        #   att = att.masked_fill(mask == 0, -1e10)\n","        #\n","        #   IMPORTANT: do this BEFORE the softmax step.\n","        #\n","        # (~5 lines of code)\n","\n","        att = torch.matmul(q, k.permute(0,2,1)) / self.scale # [batch_size, seq_len, seq_len]\n","        # Apply mask: set to a very low value if mask is zero.\n","        if mask is not None:\n","            mask = mask.unsqueeze(-1)\n","            att = att.masked_fill(mask == 0, -1e10)\n","        att = torch.nn.functional.softmax(att, dim=-1)\n","        out = torch.matmul(att, v) # [batch_size, seq_len, hidden_dim]\n","\n","        ########################\n","\n","        return out # [batch_size, seq_len, hidden_dim]\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCPMzLomsqLJ"},"source":["## Test your single head self-attention module\n","\n","Below you have a code snippet and the expected output."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z04w6PxxpxaR","executionInfo":{"status":"ok","timestamp":1673189736873,"user_tz":-60,"elapsed":343,"user":{"displayName":"David Bernal","userId":"05881219251979022189"}},"outputId":"aaab7a77-35fc-49bf-c4bc-f590aa6b1164"},"source":["torch.manual_seed(1212)\n","self_att = SelfAttention(HIDDEN_DIM)\n","emb_layer = InputEmbedding(tok.get_vocab_size(), HIDDEN_DIM)\n","enc = tok.encode('int main(int argc, char**argv[])')\n","with torch.no_grad():\n","  embs = emb_layer(torch.tensor(enc.ids)).unsqueeze(0)\n","  mask = torch.tensor(enc.attention_mask).unsqueeze(0)\n","  yhat = self_att(embs,mask)\n","  print(yhat[0])\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.2640, -0.7938,  0.1397,  ..., -0.5985, -0.2393, -0.7274],\n","        [ 0.3177, -0.9336,  0.1001,  ..., -0.6601, -0.2226, -0.7175],\n","        [ 0.2549, -0.8340,  0.0670,  ..., -0.5941, -0.1998, -0.6733],\n","        ...,\n","        [ 0.3393, -0.9621,  0.1189,  ..., -0.6528, -0.2242, -0.7558],\n","        [ 0.3393, -0.9621,  0.1189,  ..., -0.6528, -0.2242, -0.7558],\n","        [ 0.3393, -0.9621,  0.1189,  ..., -0.6528, -0.2242, -0.7558]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"5RFoEmkTldC7"},"source":["## Multi-head attention module\n","\n","This module perform a multi-head self-attention on the input tokens in\n","$x$. Is very similar to the previous class, but using multi-head.\n","\n","**IMPORTANT: DO NOT IMPLEMENT THIS CLASS YET**. Do it once you have successfully trained and evaluated a model using single-head self-attention layer.\n","\n","\n","A multihead self-attention with $k$ heads is performed in the usual way:\n","\n","$\\begin{aligned}\n","  & \\mathrm{MultiHead}(Q,K,V) = \\mathrm{Concat}(\\mathrm{head}_1,\\ldots,\\mathrm{head}_k)\\\\\n","  & \\mathrm{where}\\ \n","\\mathrm{head}_i=\\mathrm{Attention}(Q W^{Q}_i,K W^{K}_i,V W^V_i) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n","\\end{aligned}$\n","\n","and $d_k$ is the dimension of the hidden vectors.\n","\n","We won't use different $W_i$ matrices though. Instead, the module will\n","contain three linear layers as in the single head case, and change the\n","`forward` method to perform the multi-head. See below.\n","\n","## `forward` method. It receives two inputs:\n","\n","- `x` (size `batch_size` $\\times$ `seq_len` $\\times$ `hidden_dim`): ids of the subtokens. If the sequence of an instance $i$ is less than `seq_len`, the vector is padded.\n","- `mask` (size `batch_size` $\\times$ `seq_len`): a mask tensor where $m_{bi} == 0$ if the $i$th word of instance $b$ is masked. If so, the value of the logit has to be set to $-1e^{10}$.\n","\n","The output is a tensor of size (`batch_size` $\\times$ `seq_len` $\\times$ `hidden_dim`)\n","\n","**EXERCISE**: complete the `forward` function below (see the comments in the function). **REMEMBER THAT DO SHOULD IMPLEMENT THIS CLASS IN A SECOND STEP**, once you have successfully trained and evaluated a model using single-head self-attention layer."]},{"cell_type":"code","metadata":{"id":"MJP3xQb9LE54"},"source":["class MultiHeadSelfAttention(torch.nn.Module):\n","    def __init__(self, hidden_dim, n_heads):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        # hidden dimension has to be multiple of number of heads\n","        assert hidden_dim % n_heads == 0, f'hidden_dim ({hidden_dim}) is not multiple of n_heads ({n_heads})'\n","        self.n_heads = n_heads\n","        self.head_dim = hidden_dim // n_heads\n","        self.hidden_dim = hidden_dim\n","        self.Mq = torch.nn.Linear(hidden_dim, hidden_dim)\n","        self.Mk = torch.nn.Linear(hidden_dim, hidden_dim)\n","        self.Mv = torch.nn.Linear(hidden_dim, hidden_dim)\n","        self.scale = float(self.head_dim) ** 0.5 # torch.sqrt(torch.tensor(hidden_dim))\n","\n","    def forward(self, x, mask):\n","        '''Forward pass.\n","\n","        Perform a forward pass of the self-attention module. Given an input\n","        x of shape (bsz, seq_len, input_dim), apply self-attention to obtain\n","        a new vector y of shape (bsz, seq_len, hidden_dim).\n","        '''\n","        batch_size = x.size(0)\n","        q = self.Mq(x) # [batch_size, seq_len, hidden_dim]\n","        k = self.Mk(x) # [batch_size, seq_len, hidden_dim]\n","        v = self.Mv(x) # [batch_size, seq_len, hidden_dim]\n","        out = None\n","\n","        ### WRITE YOUR CODE HERE ###\n","        #\n","        #\n","        # 1. view q,v,k as [batch_size, n_heads, seq_len, head_dim] tensor\n","        # 2. obtain attention weights of size [batch size, n_heads, seq_len, seq_len]\n","        # 3. apply mask to attention weights\n","        # 4. apply softmax on the attention weights\n","        # 5. obtain out by applying attention over v to obtain a tensor of shape [batch_size, n_heads, seq_len, head_dim]\n","        # 6. set out dimensions as [batch_size, seq_len, hidden_dim]\n","        #\n","        # Useful functions:\n","        #\n","        # - torch.matmul(x,y): batch matrix multiplication\n","        # - torch.softmax(x, dim): apply softmax in the required dimension.\n","        #\n","        # Useful Tensor methods:\n","        #\n","        # - view(shape): return a new view of a tensor. For more\n","        #   information refer to the documentation:\n","        #   https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view\n","        #\n","        # - permute(shape): permute tensor to the new shape. Shape\n","        #   parameter has to be complatible with tensor shape.\n","        #   https://pytorch.org/docs/stable/generated/torch.permute.html\n","        #\n","        # - contiguous(): Returns a contiguous in memory tensor containing the same\n","        #   data as self tensor. https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html\n","        #\n","        # - masked_fill(condition, value): set value to elements that\n","        #   fullfil contition. Typical usage:\n","        #\n","        #   att = att.masked_fill(mask == 0, -1e10)\n","        #\n","        #   IMPORTANT: do this BEFORE the softmax step.\n","        # (~14 lines of code)\n","\n","        q = q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # [batch_size, n_heads, seq_len, head_dim]\n","        k = k.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # [batch_size, n_heads, seq_len, head_dim]\n","        v = v.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # [batch_size, n_heads, seq_len, head_dim]\n","        att = torch.matmul(q, k.permute(0,1,3,2)) / self.scale # [batch size, n_heads, seq_len, seq_len]\n","        # Apply mask: set to a very low value if mask is zero.\n","        if mask is not None:\n","            # mask = [batch_size, seq_len]\n","            mask = mask.unsqueeze(1).unsqueeze(1)\n","            att = att.masked_fill(mask == 0, -1e10)\n","        att = torch.softmax(att, dim=-1)\n","        out = torch.matmul(att, v) # [batch_size, n_heads, seq_len, head_dim]\n","        out = out.permute(0, 2, 1, 3).contiguous() # [batch_size, seq_len, n_heads, head_dim]\n","        out = out.view(batch_size, -1, self.hidden_dim) # [batch_size, seq_len, hidden_dim]\n","\n","        ########################\n","\n","        return out # [batch_size, seq_len, hidden_dim]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJ8GsG6Vs1t-"},"source":["## Test your multihead self-attention module\n","\n","Below you have a code snippet and the expected output.111"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVl4I3LYsnx0","executionInfo":{"status":"ok","timestamp":1636573350824,"user_tz":-60,"elapsed":184,"user":{"displayName":"Aitor Soroa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14191697461130727440"}},"outputId":"fda6df66-67fc-428e-df28-c13f3e8e0a4b"},"source":["self_att = MultiHeadSelfAttention(HIDDEN_DIM,4)\n","emb_layer = InputEmbedding(tok.get_vocab_size(), HIDDEN_DIM)\n","enc = tok.encode('int main(int argc, char**argv[])')\n","with torch.no_grad():\n","  embs = emb_layer(torch.tensor(enc.ids)).unsqueeze(0)\n","  mask = torch.tensor(enc.attention_mask).unsqueeze(0)\n","  yhat = self_att(embs,mask)\n","  print(yhat[0])\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.7262, -0.1723, -0.0943,  ...,  0.5582,  0.3762,  0.2817],\n","        [ 0.6340, -0.1306, -0.0251,  ...,  0.4880,  0.3915,  0.2166],\n","        [ 0.6994, -0.1688, -0.0754,  ...,  0.4718,  0.3981,  0.2035],\n","        ...,\n","        [ 0.6982, -0.1652, -0.0636,  ...,  0.5276,  0.3882,  0.2533],\n","        [ 0.6982, -0.1652, -0.0636,  ...,  0.5276,  0.3882,  0.2533],\n","        [ 0.6982, -0.1652, -0.0636,  ...,  0.5276,  0.3882,  0.2533]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Pw2Xycg2N5L4"},"source":["\n","## Encoder block\n","\n","We will implement the encoder like this:\n","![Encoder architecture](https://ehubox.ehu.eus/s/jRXmMpKDCTaX4tb/download)\n","\n","It consists of two submodules.\n","\n","1. The first submodule consists of:\n","   - A self-attention module (probably multi-head)\n","   - A dropout module (`torch.nn.Dropout`)\n","   - A Layer Normalization module (`torch.nn.LayerNorm`)\n","2. The second submodule consists of:\n","   - A linear layer (`torch.nn.Linear`)\n","   - A dropout module (`torch.nn.Dropout`)\n","   - A Layer Normalization module (`torch.nn.LayerNorm`)\n","\n","\n","### `forward` function:\n","\n","This function applies the encoder step to the input. See the comments in the function.\n","\n","**EXERCISE**: complete the `forward` function below.\n"]},{"cell_type":"code","metadata":{"id":"VKmIWLJNq9UC"},"source":["class EncoderBlock(torch.nn.Module):\n","    def __init__(self, hidden_dim, n_heads, dropout):\n","        super(EncoderBlock, self).__init__()\n","\n","        # First sublayer: self-attention, dropout and layer normalization\n","        if n_heads == 1:\n","            self.self_attn = SelfAttention(hidden_dim)\n","        else:\n","            self.self_attn = MultiHeadSelfAttention(hidden_dim, n_heads)\n","        self.dropout1 = torch.nn.Dropout(dropout)\n","        self.norm1 = torch.nn.LayerNorm(hidden_dim)\n","\n","        # Second sublayer: linear, dropout and layer normalization\n","        self.linear = torch.nn.Linear(hidden_dim, hidden_dim)\n","        self.ReLU = torch.nn.ReLU()\n","        self.dropout2 = torch.nn.Dropout(dropout)\n","        self.norm2 = torch.nn.LayerNorm(hidden_dim)\n","\n","    def forward(self, x, mask):\n","\n","        out = None\n","        ### WRITE YOUR CODE HERE ###\n","        #\n","        #\n","        # You have to program the following:\n","        #\n","        # s1 = LayerNorm(x + Dropout(Attention(x)))\n","        # s2 = s2 + Dropout(ReLU(Linear(s1)))\n","        # return LayerNorm(s2)\n","        #\n","        # (~6 lines of code)\n","\n","        s1 = self.self_attn(x, mask)\n","        s1 = x + self.dropout1(s1)\n","        s1 = self.norm1(s1)\n","\n","        s2 = self.ReLU(self.linear(s1))\n","        s2 = s2 + self.dropout2(s2)\n","        out = self.norm2(s2)\n","\n","        ########################\n","\n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TSUm8PNCADuW"},"source":["## PLClassifier\n","\n","This is the main class for Programming Language Classification. It is\n","composed of\n","\n","- an embedding layer (`InputEmbedding`)\n","- a dropout layer\n","- an Encoder (`Encoder`) of L layers\n","- a linear layer to classify the first embedding of each example\n","  (corresponding to the `[CLS]` special token) to the corresponding\n","  class.\n","\n","\n","### `__init__` function:\n","\n","Look carefully to the input parameters. Create the components of the\n","PLClassifier class.\n","\n","\n","### `forward` function:\n","\n","This function sends the input to the encoder, and then pools the output to classify it.\n","\n","The input is as follow:\n","\n","- `x` (size `batch_size` $\\times$ `seq_len`): ids of the (sub)tokens. If the sequence of an instance $i$ is less than `seq_len`, the vector is padded.\n","- `mask` (size `batch_size` $\\times$ `seq_len`): a mask tensor where $m_{bi} == 0$ if the $i$th word of instance $b$ is masked.\n","\n","The output is a tensor of size (`batch_size` $\\times$ `n_classes`)\n","\n","To produce the output, the function does the following:\n","\n","1. Obtain embeddings input embeddings for x\n","2. Apply a dropout\n","3. Send the input to the encoder\n","4. Pool the output to obtain the first embedding first embedding of\n","   each example (corresponding to the `[CLS]` special token), and send\n","   it to the linear module.\n","\n","**EXERCISE**: complete the `__init__` and `forward` functions below.\n"]},{"cell_type":"code","metadata":{"id":"LnlAYR8_AEM2"},"source":["class PLClassifier(torch.nn.Module):\n","    def __init__(self, n_layers, hidden_dim, heads,\n","                 vocab_size, n_classes, dropout = DROPOUT_PROB):\n","        super(PLClassifier, self).__init__()\n","\n","        self.embeddings = None\n","        self.dropout = None\n","        self.encoders = None\n","        self.fc = None\n","\n","        ### WRITE YOUR CODE HERE ###\n","        # Note:\n","        # 1. You should use torch.nn.ModuleList for self.encoders\n","        # 2. self.encoders has n_layers EncoderBlock layers\n","        # 3. You should use torch.nn.Dropout for self.dropout\n","        #\n","        # (~5 lines of code)\n","\n","        self.embeddings = InputEmbedding(vocab_size, hidden_dim)\n","        self.encoders = torch.nn.ModuleList([EncoderBlock(hidden_dim, heads, dropout)\n","                                             for _ in range(n_layers)])\n","        self.fc = torch.nn.Linear(hidden_dim, n_classes)\n","        self.dropout = torch.nn.Dropout(dropout)\n","\n","        ########################\n","\n","    def forward(self, x, mask):\n","\n","        # x [batch_size, seq_len]\n","\n","        out = None\n","        ### WRITE YOUR CODE HERE ###\n","        # Note:\n","        # 1. Obtain embeddings input embeddings for x\n","        # 2. Apply a dropout\n","        # 3. Send the input to the encoder\n","        # 4. Use the first embedding of each instance (corresp. to\n","        #   '[CLS]') and send it to the linear layer\n","        #\n","        # (~6 lines of code)\n","\n","        x = self.embeddings(x)\n","        x = self.dropout(x)\n","        for encoder in self.encoders:\n","            x = encoder(x, mask)\n","        # get pooled embedding (CLS)\n","        x = x[:,0,:]\n","        out = self.fc(x)\n","\n","        ########################\n","\n","        return out # (batch_size, n_classes)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DQDh82pNAJSe"},"source":["## Training the model\n","\n","Nothing special here, just a typical training loop.\n","\n","**EXERCISE**: complete the `training_step` function below.\n"]},{"cell_type":"code","metadata":{"id":"r8lR7LNlALRg"},"source":["def training_step(model, train_loader, loss_fn, optimizer, epoch, log_every):\n","    '''Train the model'''\n","\n","    model.train() # training mode (for batchnorm, dropout, etc)\n","    for i, (sents, masks, labels) in enumerate(train_loader):\n","        sents = sents.to(device)\n","        masks = masks.to(device)\n","        labels = labels.to(device)\n","\n","        ############# Your code here ############\n","        # Note:\n","        # 1. Zero grad the optimizer\n","        # 2. Feed the data into the model\n","        # 4. Feed the output and label to loss_fn\n","        # (~3 lines of code)\n","\n","        optimizer.zero_grad()\n","        outputs = model(sents, masks)\n","        loss = loss_fn(outputs, labels.squeeze(-1))\n","\n","        ########################\n","        # backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","        if (i+1) % log_every == 0:\n","            print (f'Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.8f}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SYfyILhSAQlQ"},"source":["def evaluate(model, loader):\n","    ok = 0\n","    total = 0\n","    loss_list = []\n","    criterion = torch.nn.CrossEntropyLoss() # combines LogSoftmax and NLLLoss\n","    model.eval()\n","    with torch.no_grad(): # do not store gradients (reduces memory consumption)\n","        for i, (sents, masks, labels) in enumerate(loader):\n","            sents = sents.to(device)\n","            masks = masks.to(device)\n","            labels = labels.to(device)\n","            outputs = model(sents, masks)\n","            loss = criterion(outputs, labels.squeeze(-1))\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            loss_list.append(loss.item())\n","            ok += (predicted == labels.squeeze(-1)).sum().item()\n","    model.train() # return to training mode (for batchnorm, dropout, etc)\n","    return sum(loss_list)/len(loss_list), 100*ok/total\n","\n","def train(model, num_epochs, train_loader, val_loader, test_model, writer = None):\n","    '''Train the model'''\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    loss_fn = torch.nn.CrossEntropyLoss() # combines LogSoftmax and NLLLoss\n","    steps_per_epoch = len(train_loader)\n","    log_every = min(15, steps_per_epoch)\n","    for epoch in range(num_epochs):\n","        training_step(model, train_loader, loss_fn, optimizer, epoch, log_every)\n","        current_step = (epoch + 1) * steps_per_epoch\n","        train_loss, train_acc = evaluate(model, train_loader)\n","        val_loss, val_acc = evaluate(model, val_loader)\n","        test_loss, test_acc = evaluate(model, test_loader)\n","        if writer is not None:\n","            writer.add_scalar('training loss', train_loss, current_step)\n","            writer.add_scalar('training acc', train_acc, current_step)\n","            writer.add_scalar('validation loss', val_loss, current_step)\n","            writer.add_scalar('validation acc', val_acc, current_step)\n","        print(f'Evaluating model in epoch {epoch+1} Val loss/acc: {val_loss:.8f}|{val_acc:.4f} Test loss/acc: {test_loss:.8f}|{test_acc:.4f}')\n","\n","def predict(model, loader):\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad(): # do not store gradients (reduces memory consumption)\n","        for i, (sents, masks, labels) in enumerate(loader):\n","            sents = sents.to(device)\n","            masks = masks.to(device)\n","            labels = labels.to(device)\n","            outputs = model(sents, masks)\n","            _, predicted = torch.max(outputs.data, 1)\n","            predictions.extend(predicted.tolist())\n","    return predictions\n","\n","# load dataset\n","\n","tok = Tokenizer.from_file(\"mlnn_lab5.4/tokenizer.json\")\n","tok.enable_padding(direction='right',length=SEQ_LEN) # max sequence is 150\n","\n","train_dataset = Dataset('mlnn_lab5.4/train.txt', tok)\n","val_dataset = Dataset('mlnn_lab5.4/dev.txt', tok)\n","test_dataset = Dataset('mlnn_lab5.4/test.txt',tok)\n","\n","# dataloaders automatically shuffle the data and create batches, using the 'collate_fn' function\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,\n","                                           shuffle=True)\n","val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE,\n","                                         shuffle=False)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE,\n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Juz_ydfrAdyp"},"source":["## Create a single head model, train and evaluate.\n","\n"," You should obtain a test accuracy of\n"," - ~33.6% with 1 epoch\n"," - ~46.8% with 2 epochs\n"," - ~69.7% with 20 epochs\n"," - ~72.5% with 50 epochs\n","\n","Compute only the first two epochs during class hours (you can stop the execution afterwards), as it takes almost an hour to train the model for 50 epochs and you should see if the network is learning correctly the classification task by the end of the second epoch."]},{"cell_type":"code","metadata":{"id":"2NTZPibFAgtm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"40613de4-adcb-4d3d-d82d-07cf8f95a27a"},"source":["model=PLClassifier(NUM_LAYERS, HIDDEN_DIM, 1, tok.get_vocab_size(), N_LABELS)\n","model=model.to(device)\n","\n","logdir = \"logs/singlehead_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","writer = SummaryWriter(logdir)\n","\n","train(model, NUM_EPOCHS, train_loader, val_loader, test_loader, writer)\n","writer.flush()\n","\n","# evaluate the model\n","test_loss, test_acc = evaluate(model, test_loader)\n","print(f'Test loss/accuracy: {test_loss:.4f}/{test_acc:.4f}')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/50], Step [15/164], Loss: 3.07382131\n","Epoch [1/50], Step [30/164], Loss: 3.00515079\n","Epoch [1/50], Step [45/164], Loss: 2.99736524\n","Epoch [1/50], Step [60/164], Loss: 2.92004418\n","Epoch [1/50], Step [75/164], Loss: 2.79229522\n","Epoch [1/50], Step [90/164], Loss: 2.75135708\n","Epoch [1/50], Step [105/164], Loss: 2.68686366\n","Epoch [1/50], Step [120/164], Loss: 2.65278411\n","Epoch [1/50], Step [135/164], Loss: 2.56336188\n","Epoch [1/50], Step [150/164], Loss: 2.44470954\n","Evaluating model in epoch 1 Val loss/acc: 2.25454913|34.6925 Test loss/acc: 2.26346034|33.6668\n","Epoch [2/50], Step [15/164], Loss: 2.31197572\n","Epoch [2/50], Step [30/164], Loss: 2.31117678\n","Epoch [2/50], Step [45/164], Loss: 2.16237593\n","Epoch [2/50], Step [60/164], Loss: 2.12400150\n","Epoch [2/50], Step [75/164], Loss: 2.14129162\n","Epoch [2/50], Step [90/164], Loss: 2.13652730\n","Epoch [2/50], Step [105/164], Loss: 1.99963379\n","Epoch [2/50], Step [120/164], Loss: 1.97851324\n","Epoch [2/50], Step [135/164], Loss: 1.96936226\n","Epoch [2/50], Step [150/164], Loss: 1.94952440\n","Evaluating model in epoch 2 Val loss/acc: 1.80986127|48.2674 Test loss/acc: 1.85449357|46.8734\n","Epoch [3/50], Step [15/164], Loss: 1.90320027\n","Epoch [3/50], Step [30/164], Loss: 1.81356108\n","Epoch [3/50], Step [45/164], Loss: 1.74652636\n","Epoch [3/50], Step [60/164], Loss: 1.75632620\n","Epoch [3/50], Step [75/164], Loss: 1.82292604\n","Epoch [3/50], Step [90/164], Loss: 1.77380586\n","Epoch [3/50], Step [105/164], Loss: 1.75212812\n","Epoch [3/50], Step [120/164], Loss: 1.75828755\n","Epoch [3/50], Step [135/164], Loss: 1.73968673\n","Epoch [3/50], Step [150/164], Loss: 1.79798090\n","Evaluating model in epoch 3 Val loss/acc: 1.59255733|55.1091 Test loss/acc: 1.65732482|52.6763\n","Epoch [4/50], Step [15/164], Loss: 1.70186698\n","Epoch [4/50], Step [30/164], Loss: 1.70543146\n","Epoch [4/50], Step [45/164], Loss: 1.54307008\n","Epoch [4/50], Step [60/164], Loss: 1.65456545\n","Epoch [4/50], Step [75/164], Loss: 1.58066988\n","Epoch [4/50], Step [90/164], Loss: 1.66026449\n","Epoch [4/50], Step [105/164], Loss: 1.59331632\n","Epoch [4/50], Step [120/164], Loss: 1.64705992\n","Epoch [4/50], Step [135/164], Loss: 1.60160744\n","Epoch [4/50], Step [150/164], Loss: 1.61008799\n","Evaluating model in epoch 4 Val loss/acc: 1.46615368|58.2288 Test loss/acc: 1.52178636|56.9785\n","Epoch [5/50], Step [15/164], Loss: 1.52952218\n","Epoch [5/50], Step [30/164], Loss: 1.51803589\n","Epoch [5/50], Step [45/164], Loss: 1.56374800\n","Epoch [5/50], Step [60/164], Loss: 1.68734014\n","Epoch [5/50], Step [75/164], Loss: 1.53042340\n","Epoch [5/50], Step [90/164], Loss: 1.50002456\n","Epoch [5/50], Step [105/164], Loss: 1.44529021\n","Epoch [5/50], Step [120/164], Loss: 1.46977198\n","Epoch [5/50], Step [135/164], Loss: 1.42876768\n","Epoch [5/50], Step [150/164], Loss: 1.51260328\n","Evaluating model in epoch 5 Val loss/acc: 1.37206946|60.8945 Test loss/acc: 1.43723476|59.2296\n","Epoch [6/50], Step [15/164], Loss: 1.62283731\n","Epoch [6/50], Step [30/164], Loss: 1.38764405\n","Epoch [6/50], Step [45/164], Loss: 1.27791107\n","Epoch [6/50], Step [60/164], Loss: 1.43482208\n","Epoch [6/50], Step [75/164], Loss: 1.35987151\n","Epoch [6/50], Step [90/164], Loss: 1.38456762\n","Epoch [6/50], Step [105/164], Loss: 1.44562876\n","Epoch [6/50], Step [120/164], Loss: 1.31531453\n","Epoch [6/50], Step [135/164], Loss: 1.44280839\n","Epoch [6/50], Step [150/164], Loss: 1.53007948\n","Evaluating model in epoch 6 Val loss/acc: 1.30107774|63.0171 Test loss/acc: 1.36198927|60.9305\n","Epoch [7/50], Step [15/164], Loss: 1.46374750\n","Epoch [7/50], Step [30/164], Loss: 1.31241107\n","Epoch [7/50], Step [45/164], Loss: 1.26566076\n","Epoch [7/50], Step [60/164], Loss: 1.34942114\n","Epoch [7/50], Step [75/164], Loss: 1.33627164\n","Epoch [7/50], Step [90/164], Loss: 1.34138775\n","Epoch [7/50], Step [105/164], Loss: 1.29558063\n","Epoch [7/50], Step [120/164], Loss: 1.53680623\n","Epoch [7/50], Step [135/164], Loss: 1.30777407\n","Epoch [7/50], Step [150/164], Loss: 1.41447115\n","Evaluating model in epoch 7 Val loss/acc: 1.25445831|64.2709 Test loss/acc: 1.30916908|63.1816\n","Epoch [8/50], Step [15/164], Loss: 1.44317710\n","Epoch [8/50], Step [30/164], Loss: 1.25962389\n","Epoch [8/50], Step [45/164], Loss: 1.29961085\n","Epoch [8/50], Step [60/164], Loss: 1.26217234\n","Epoch [8/50], Step [75/164], Loss: 1.21026444\n","Epoch [8/50], Step [90/164], Loss: 1.49786603\n","Epoch [8/50], Step [105/164], Loss: 1.24010193\n","Epoch [8/50], Step [120/164], Loss: 1.29413378\n","Epoch [8/50], Step [135/164], Loss: 1.19679332\n","Epoch [8/50], Step [150/164], Loss: 1.26044357\n","Evaluating model in epoch 8 Val loss/acc: 1.20126176|65.7321 Test loss/acc: 1.25639372|63.6318\n","Epoch [9/50], Step [15/164], Loss: 1.31960249\n","Epoch [9/50], Step [30/164], Loss: 1.04209602\n","Epoch [9/50], Step [45/164], Loss: 1.19284546\n","Epoch [9/50], Step [60/164], Loss: 1.22450268\n","Epoch [9/50], Step [75/164], Loss: 1.22356105\n","Epoch [9/50], Step [90/164], Loss: 1.17730093\n","Epoch [9/50], Step [105/164], Loss: 1.25479317\n","Epoch [9/50], Step [120/164], Loss: 1.31376779\n","Epoch [9/50], Step [135/164], Loss: 1.21699309\n","Epoch [9/50], Step [150/164], Loss: 1.27591527\n","Evaluating model in epoch 9 Val loss/acc: 1.17309897|66.3837 Test loss/acc: 1.22569710|64.2321\n","Epoch [10/50], Step [15/164], Loss: 1.11014247\n","Epoch [10/50], Step [30/164], Loss: 1.22479403\n","Epoch [10/50], Step [45/164], Loss: 1.30335009\n","Epoch [10/50], Step [60/164], Loss: 1.20129943\n","Epoch [10/50], Step [75/164], Loss: 1.14008689\n","Epoch [10/50], Step [90/164], Loss: 1.23043036\n","Epoch [10/50], Step [105/164], Loss: 1.21162045\n","Epoch [10/50], Step [120/164], Loss: 1.18141353\n","Epoch [10/50], Step [135/164], Loss: 1.06027091\n","Epoch [10/50], Step [150/164], Loss: 1.03225291\n","Evaluating model in epoch 10 Val loss/acc: 1.14530201|67.1932 Test loss/acc: 1.19398733|64.7824\n","Epoch [11/50], Step [15/164], Loss: 1.12199461\n","Epoch [11/50], Step [30/164], Loss: 1.21028996\n","Epoch [11/50], Step [45/164], Loss: 1.16355217\n","Epoch [11/50], Step [60/164], Loss: 1.27994990\n","Epoch [11/50], Step [75/164], Loss: 1.19438457\n","Epoch [11/50], Step [90/164], Loss: 0.96788949\n","Epoch [11/50], Step [105/164], Loss: 1.18311930\n","Epoch [11/50], Step [120/164], Loss: 1.12085998\n","Epoch [11/50], Step [135/164], Loss: 1.11990249\n","Epoch [11/50], Step [150/164], Loss: 1.23336828\n","Evaluating model in epoch 11 Val loss/acc: 1.13235465|67.8843 Test loss/acc: 1.17862591|65.4827\n","Epoch [12/50], Step [15/164], Loss: 1.03756499\n","Epoch [12/50], Step [30/164], Loss: 1.12798083\n","Epoch [12/50], Step [45/164], Loss: 1.19129145\n","Epoch [12/50], Step [60/164], Loss: 1.09196031\n","Epoch [12/50], Step [75/164], Loss: 1.07268786\n","Epoch [12/50], Step [90/164], Loss: 1.17144835\n","Epoch [12/50], Step [105/164], Loss: 1.02946591\n","Epoch [12/50], Step [120/164], Loss: 1.20105445\n","Epoch [12/50], Step [135/164], Loss: 1.07685184\n","Epoch [12/50], Step [150/164], Loss: 1.23429680\n","Evaluating model in epoch 12 Val loss/acc: 1.12511220|67.9633 Test loss/acc: 1.17304298|66.3832\n","Epoch [13/50], Step [15/164], Loss: 1.02018344\n","Epoch [13/50], Step [30/164], Loss: 1.04351282\n","Epoch [13/50], Step [45/164], Loss: 1.07819664\n","Epoch [13/50], Step [60/164], Loss: 1.11725378\n","Epoch [13/50], Step [75/164], Loss: 1.02042389\n","Epoch [13/50], Step [90/164], Loss: 1.24678445\n","Epoch [13/50], Step [105/164], Loss: 1.28068602\n","Epoch [13/50], Step [120/164], Loss: 1.13660300\n","Epoch [13/50], Step [135/164], Loss: 1.19908535\n","Epoch [13/50], Step [150/164], Loss: 1.11900496\n","Evaluating model in epoch 13 Val loss/acc: 1.10012700|69.0295 Test loss/acc: 1.14671517|66.5833\n","Epoch [14/50], Step [15/164], Loss: 1.03830445\n","Epoch [14/50], Step [30/164], Loss: 1.08375168\n","Epoch [14/50], Step [45/164], Loss: 1.01722836\n","Epoch [14/50], Step [60/164], Loss: 1.23870087\n","Epoch [14/50], Step [75/164], Loss: 1.02484381\n","Epoch [14/50], Step [90/164], Loss: 1.09388840\n","Epoch [14/50], Step [105/164], Loss: 1.15614915\n","Epoch [14/50], Step [120/164], Loss: 1.06412077\n","Epoch [14/50], Step [135/164], Loss: 1.13720334\n","Epoch [14/50], Step [150/164], Loss: 1.07738662\n","Evaluating model in epoch 14 Val loss/acc: 1.08885252|69.1184 Test loss/acc: 1.13470123|67.4337\n","Epoch [15/50], Step [15/164], Loss: 1.15507007\n","Epoch [15/50], Step [30/164], Loss: 1.09397066\n","Epoch [15/50], Step [45/164], Loss: 1.17236698\n","Epoch [15/50], Step [60/164], Loss: 1.06280494\n","Epoch [15/50], Step [75/164], Loss: 1.14985847\n","Epoch [15/50], Step [90/164], Loss: 1.05037665\n","Epoch [15/50], Step [105/164], Loss: 1.00091970\n","Epoch [15/50], Step [120/164], Loss: 1.11006880\n","Epoch [15/50], Step [135/164], Loss: 1.03068066\n","Epoch [15/50], Step [150/164], Loss: 1.08545101\n","Evaluating model in epoch 15 Val loss/acc: 1.08809866|69.2862 Test loss/acc: 1.12591033|67.8839\n","Epoch [16/50], Step [15/164], Loss: 1.12663519\n","Epoch [16/50], Step [30/164], Loss: 1.02584624\n","Epoch [16/50], Step [45/164], Loss: 0.94812608\n","Epoch [16/50], Step [60/164], Loss: 1.02021205\n","Epoch [16/50], Step [75/164], Loss: 1.08937991\n","Epoch [16/50], Step [90/164], Loss: 1.08821893\n","Epoch [16/50], Step [105/164], Loss: 1.07122397\n","Epoch [16/50], Step [120/164], Loss: 0.98605490\n","Epoch [16/50], Step [135/164], Loss: 1.12596631\n","Epoch [16/50], Step [150/164], Loss: 1.15090799\n","Evaluating model in epoch 16 Val loss/acc: 1.07792860|69.4639 Test loss/acc: 1.11403999|68.1341\n","Epoch [17/50], Step [15/164], Loss: 1.09275484\n","Epoch [17/50], Step [30/164], Loss: 0.91909355\n","Epoch [17/50], Step [45/164], Loss: 0.95082438\n","Epoch [17/50], Step [60/164], Loss: 1.07839668\n","Epoch [17/50], Step [75/164], Loss: 1.03522193\n","Epoch [17/50], Step [90/164], Loss: 0.97583759\n","Epoch [17/50], Step [105/164], Loss: 0.99246216\n","Epoch [17/50], Step [120/164], Loss: 1.08094883\n","Epoch [17/50], Step [135/164], Loss: 1.08607602\n","Epoch [17/50], Step [150/164], Loss: 1.05546486\n","Evaluating model in epoch 17 Val loss/acc: 1.06415145|70.0563 Test loss/acc: 1.10757387|68.5843\n","Epoch [18/50], Step [15/164], Loss: 1.03390861\n","Epoch [18/50], Step [30/164], Loss: 0.95470345\n","Epoch [18/50], Step [45/164], Loss: 0.99027354\n","Epoch [18/50], Step [60/164], Loss: 1.10755610\n","Epoch [18/50], Step [75/164], Loss: 1.07320678\n","Epoch [18/50], Step [90/164], Loss: 0.99542898\n","Epoch [18/50], Step [105/164], Loss: 1.03014052\n","Epoch [18/50], Step [120/164], Loss: 1.16833341\n","Epoch [18/50], Step [135/164], Loss: 1.12963223\n","Epoch [18/50], Step [150/164], Loss: 0.97425401\n","Evaluating model in epoch 18 Val loss/acc: 1.05657198|70.1451 Test loss/acc: 1.09809850|69.0345\n","Epoch [19/50], Step [15/164], Loss: 0.86340052\n","Epoch [19/50], Step [30/164], Loss: 0.99442941\n","Epoch [19/50], Step [45/164], Loss: 0.99274069\n","Epoch [19/50], Step [60/164], Loss: 0.89093906\n","Epoch [19/50], Step [75/164], Loss: 1.01469266\n","Epoch [19/50], Step [90/164], Loss: 1.00014627\n","Epoch [19/50], Step [105/164], Loss: 0.94593370\n","Epoch [19/50], Step [120/164], Loss: 1.07728601\n","Epoch [19/50], Step [135/164], Loss: 1.13969672\n","Epoch [19/50], Step [150/164], Loss: 0.98316073\n","Evaluating model in epoch 19 Val loss/acc: 1.05120770|70.3623 Test loss/acc: 1.08344411|69.5348\n","Epoch [20/50], Step [15/164], Loss: 0.93133569\n","Epoch [20/50], Step [30/164], Loss: 0.98756087\n","Epoch [20/50], Step [45/164], Loss: 0.98623276\n","Epoch [20/50], Step [60/164], Loss: 1.01569402\n","Epoch [20/50], Step [75/164], Loss: 1.07463396\n","Epoch [20/50], Step [90/164], Loss: 1.00001693\n","Epoch [20/50], Step [105/164], Loss: 0.93502176\n","Epoch [20/50], Step [120/164], Loss: 1.12821257\n","Epoch [20/50], Step [135/164], Loss: 0.90391403\n","Epoch [20/50], Step [150/164], Loss: 0.87903291\n","Evaluating model in epoch 20 Val loss/acc: 1.04182660|70.9349 Test loss/acc: 1.06972066|69.7349\n","Epoch [21/50], Step [15/164], Loss: 0.97140002\n","Epoch [21/50], Step [30/164], Loss: 0.99991405\n","Epoch [21/50], Step [45/164], Loss: 1.08706260\n","Epoch [21/50], Step [60/164], Loss: 0.97398418\n","Epoch [21/50], Step [75/164], Loss: 0.95030648\n","Epoch [21/50], Step [90/164], Loss: 0.84249783\n","Epoch [21/50], Step [105/164], Loss: 0.92085993\n","Epoch [21/50], Step [120/164], Loss: 0.97616726\n","Epoch [21/50], Step [135/164], Loss: 0.91610855\n","Epoch [21/50], Step [150/164], Loss: 1.05759490\n","Evaluating model in epoch 21 Val loss/acc: 1.03442708|70.9251 Test loss/acc: 1.06022409|70.1851\n","Epoch [22/50], Step [15/164], Loss: 0.87819451\n","Epoch [22/50], Step [30/164], Loss: 0.93311518\n","Epoch [22/50], Step [45/164], Loss: 0.98841733\n","Epoch [22/50], Step [60/164], Loss: 1.01170194\n","Epoch [22/50], Step [75/164], Loss: 0.81000537\n","Epoch [22/50], Step [90/164], Loss: 0.84662747\n","Epoch [22/50], Step [105/164], Loss: 0.85415906\n","Epoch [22/50], Step [120/164], Loss: 0.98879099\n","Epoch [22/50], Step [135/164], Loss: 0.90278184\n","Epoch [22/50], Step [150/164], Loss: 0.86480868\n","Evaluating model in epoch 22 Val loss/acc: 1.02885936|71.1620 Test loss/acc: 1.06424205|69.7349\n","Epoch [23/50], Step [15/164], Loss: 0.93930703\n","Epoch [23/50], Step [30/164], Loss: 0.80409122\n","Epoch [23/50], Step [45/164], Loss: 0.96907204\n","Epoch [23/50], Step [60/164], Loss: 0.99741459\n","Epoch [23/50], Step [75/164], Loss: 0.93263495\n","Epoch [23/50], Step [90/164], Loss: 0.92404634\n","Epoch [23/50], Step [105/164], Loss: 0.89532709\n","Epoch [23/50], Step [120/164], Loss: 1.00421691\n","Epoch [23/50], Step [135/164], Loss: 0.88373137\n","Epoch [23/50], Step [150/164], Loss: 0.78943872\n","Evaluating model in epoch 23 Val loss/acc: 1.02590936|71.1423 Test loss/acc: 1.05524723|69.5348\n","Epoch [24/50], Step [15/164], Loss: 0.86660314\n","Epoch [24/50], Step [30/164], Loss: 0.91267920\n","Epoch [24/50], Step [45/164], Loss: 0.88919663\n","Epoch [24/50], Step [60/164], Loss: 0.73761898\n","Epoch [24/50], Step [75/164], Loss: 0.84730899\n","Epoch [24/50], Step [90/164], Loss: 1.07369328\n","Epoch [24/50], Step [105/164], Loss: 0.85033250\n","Epoch [24/50], Step [120/164], Loss: 0.88941461\n","Epoch [24/50], Step [135/164], Loss: 0.90561545\n","Epoch [24/50], Step [150/164], Loss: 0.84431553\n","Evaluating model in epoch 24 Val loss/acc: 1.01554598|71.4187 Test loss/acc: 1.03914605|70.1851\n","Epoch [25/50], Step [15/164], Loss: 0.72240895\n","Epoch [25/50], Step [30/164], Loss: 0.93583030\n","Epoch [25/50], Step [45/164], Loss: 0.90132284\n","Epoch [25/50], Step [60/164], Loss: 1.02686787\n","Epoch [25/50], Step [75/164], Loss: 0.94487262\n","Epoch [25/50], Step [90/164], Loss: 0.88297164\n","Epoch [25/50], Step [105/164], Loss: 0.93736827\n","Epoch [25/50], Step [120/164], Loss: 0.92175293\n","Epoch [25/50], Step [135/164], Loss: 1.00146675\n","Epoch [25/50], Step [150/164], Loss: 0.87555730\n","Evaluating model in epoch 25 Val loss/acc: 1.01801081|71.8333 Test loss/acc: 1.04008333|70.1351\n","Epoch [26/50], Step [15/164], Loss: 0.75941980\n","Epoch [26/50], Step [30/164], Loss: 0.95410651\n","Epoch [26/50], Step [45/164], Loss: 1.00524545\n","Epoch [26/50], Step [60/164], Loss: 0.72623926\n","Epoch [26/50], Step [75/164], Loss: 0.92529905\n","Epoch [26/50], Step [90/164], Loss: 0.74363846\n","Epoch [26/50], Step [105/164], Loss: 0.85113215\n","Epoch [26/50], Step [120/164], Loss: 0.92080319\n","Epoch [26/50], Step [135/164], Loss: 0.98546839\n","Epoch [26/50], Step [150/164], Loss: 0.98338640\n","Evaluating model in epoch 26 Val loss/acc: 1.01155909|71.9814 Test loss/acc: 1.02995115|70.5853\n","Epoch [27/50], Step [15/164], Loss: 1.01981258\n","Epoch [27/50], Step [30/164], Loss: 0.84114057\n","Epoch [27/50], Step [45/164], Loss: 0.84699064\n","Epoch [27/50], Step [60/164], Loss: 0.85999364\n","Epoch [27/50], Step [75/164], Loss: 0.83127123\n","Epoch [27/50], Step [90/164], Loss: 0.72803593\n","Epoch [27/50], Step [105/164], Loss: 0.77391511\n","Epoch [27/50], Step [120/164], Loss: 0.96345896\n","Epoch [27/50], Step [135/164], Loss: 0.96045768\n","Epoch [27/50], Step [150/164], Loss: 0.98582983\n","Evaluating model in epoch 27 Val loss/acc: 1.00491636|71.9419 Test loss/acc: 1.02707236|70.7354\n","Epoch [28/50], Step [15/164], Loss: 0.88984489\n","Epoch [28/50], Step [30/164], Loss: 0.79752791\n","Epoch [28/50], Step [45/164], Loss: 0.92958289\n","Epoch [28/50], Step [60/164], Loss: 0.94255936\n","Epoch [28/50], Step [75/164], Loss: 0.96104044\n","Epoch [28/50], Step [90/164], Loss: 0.81425655\n","Epoch [28/50], Step [105/164], Loss: 0.87510288\n","Epoch [28/50], Step [120/164], Loss: 0.79538810\n","Epoch [28/50], Step [135/164], Loss: 0.91571611\n","Epoch [28/50], Step [150/164], Loss: 0.84679341\n","Evaluating model in epoch 28 Val loss/acc: 1.00799601|71.8630 Test loss/acc: 1.02250299|70.9855\n","Epoch [29/50], Step [15/164], Loss: 0.83438545\n","Epoch [29/50], Step [30/164], Loss: 0.88851947\n","Epoch [29/50], Step [45/164], Loss: 0.83526409\n","Epoch [29/50], Step [60/164], Loss: 1.12268734\n","Epoch [29/50], Step [75/164], Loss: 0.87023783\n","Epoch [29/50], Step [90/164], Loss: 0.85591233\n","Epoch [29/50], Step [105/164], Loss: 0.81469160\n","Epoch [29/50], Step [120/164], Loss: 0.79412210\n","Epoch [29/50], Step [135/164], Loss: 0.94052941\n","Epoch [29/50], Step [150/164], Loss: 0.74614859\n","Evaluating model in epoch 29 Val loss/acc: 1.00909891|71.9222 Test loss/acc: 1.02452936|70.4852\n","Epoch [30/50], Step [15/164], Loss: 0.78805852\n","Epoch [30/50], Step [30/164], Loss: 0.90771556\n","Epoch [30/50], Step [45/164], Loss: 0.78046381\n","Epoch [30/50], Step [60/164], Loss: 0.75860572\n","Epoch [30/50], Step [75/164], Loss: 0.92844963\n","Epoch [30/50], Step [90/164], Loss: 0.94276381\n","Epoch [30/50], Step [105/164], Loss: 0.76099122\n","Epoch [30/50], Step [120/164], Loss: 0.73674053\n","Epoch [30/50], Step [135/164], Loss: 0.83153063\n","Epoch [30/50], Step [150/164], Loss: 0.84977251\n","Evaluating model in epoch 30 Val loss/acc: 1.01363114|71.9617 Test loss/acc: 1.01772765|71.1356\n","Epoch [31/50], Step [15/164], Loss: 0.81085765\n","Epoch [31/50], Step [30/164], Loss: 0.85188460\n","Epoch [31/50], Step [45/164], Loss: 0.91855830\n","Epoch [31/50], Step [60/164], Loss: 0.81220520\n","Epoch [31/50], Step [75/164], Loss: 1.03967035\n","Epoch [31/50], Step [90/164], Loss: 0.92300570\n","Epoch [31/50], Step [105/164], Loss: 0.89237136\n","Epoch [31/50], Step [120/164], Loss: 0.84825319\n","Epoch [31/50], Step [135/164], Loss: 0.84530669\n","Epoch [31/50], Step [150/164], Loss: 0.77904117\n","Evaluating model in epoch 31 Val loss/acc: 1.00298604|72.1789 Test loss/acc: 1.00739427|70.9855\n","Epoch [32/50], Step [15/164], Loss: 0.81313902\n","Epoch [32/50], Step [30/164], Loss: 0.94709754\n","Epoch [32/50], Step [45/164], Loss: 0.89005119\n","Epoch [32/50], Step [60/164], Loss: 0.77918702\n","Epoch [32/50], Step [75/164], Loss: 0.85506845\n","Epoch [32/50], Step [90/164], Loss: 0.81476349\n","Epoch [32/50], Step [105/164], Loss: 0.88976574\n","Epoch [32/50], Step [120/164], Loss: 0.97280127\n","Epoch [32/50], Step [135/164], Loss: 0.81424046\n","Epoch [32/50], Step [150/164], Loss: 0.69305760\n","Evaluating model in epoch 32 Val loss/acc: 1.00521103|72.5146 Test loss/acc: 1.00728731|71.4857\n","Epoch [33/50], Step [15/164], Loss: 0.78113842\n","Epoch [33/50], Step [30/164], Loss: 0.69234741\n","Epoch [33/50], Step [45/164], Loss: 0.98398578\n","Epoch [33/50], Step [60/164], Loss: 0.95774567\n","Epoch [33/50], Step [75/164], Loss: 0.87773412\n","Epoch [33/50], Step [90/164], Loss: 0.81231225\n","Epoch [33/50], Step [105/164], Loss: 0.86487532\n","Epoch [33/50], Step [120/164], Loss: 0.82912242\n","Epoch [33/50], Step [135/164], Loss: 0.83591425\n","Epoch [33/50], Step [150/164], Loss: 0.75673383\n","Evaluating model in epoch 33 Val loss/acc: 1.00232187|72.3270 Test loss/acc: 1.00911236|71.7359\n","Epoch [34/50], Step [15/164], Loss: 0.85922641\n","Epoch [34/50], Step [30/164], Loss: 0.92231101\n","Epoch [34/50], Step [45/164], Loss: 0.73672086\n","Epoch [34/50], Step [60/164], Loss: 0.88893896\n","Epoch [34/50], Step [75/164], Loss: 0.95263350\n","Epoch [34/50], Step [90/164], Loss: 0.75644952\n","Epoch [34/50], Step [105/164], Loss: 0.75365293\n","Epoch [34/50], Step [120/164], Loss: 0.80886614\n","Epoch [34/50], Step [135/164], Loss: 0.81443763\n","Epoch [34/50], Step [150/164], Loss: 0.93961871\n","Evaluating model in epoch 34 Val loss/acc: 0.99880393|72.4060 Test loss/acc: 1.00431944|71.5858\n","Epoch [35/50], Step [15/164], Loss: 0.82378006\n","Epoch [35/50], Step [30/164], Loss: 0.72974437\n","Epoch [35/50], Step [45/164], Loss: 0.85514069\n","Epoch [35/50], Step [60/164], Loss: 0.72320819\n","Epoch [35/50], Step [75/164], Loss: 0.86168391\n","Epoch [35/50], Step [90/164], Loss: 0.79230636\n","Epoch [35/50], Step [105/164], Loss: 0.91341335\n","Epoch [35/50], Step [120/164], Loss: 0.79491961\n","Epoch [35/50], Step [135/164], Loss: 0.84492046\n","Epoch [35/50], Step [150/164], Loss: 0.73832023\n","Evaluating model in epoch 35 Val loss/acc: 0.99870533|72.5837 Test loss/acc: 1.00567603|72.1861\n","Epoch [36/50], Step [15/164], Loss: 0.63540417\n","Epoch [36/50], Step [30/164], Loss: 0.88690501\n","Epoch [36/50], Step [45/164], Loss: 0.89583671\n","Epoch [36/50], Step [60/164], Loss: 0.84604740\n","Epoch [36/50], Step [75/164], Loss: 0.79950094\n","Epoch [36/50], Step [90/164], Loss: 0.70171410\n","Epoch [36/50], Step [105/164], Loss: 0.84209156\n","Epoch [36/50], Step [120/164], Loss: 0.78133386\n","Epoch [36/50], Step [135/164], Loss: 0.79112554\n","Epoch [36/50], Step [150/164], Loss: 0.74347502\n","Evaluating model in epoch 36 Val loss/acc: 0.99750736|72.5244 Test loss/acc: 0.99207912|72.7364\n","Epoch [37/50], Step [15/164], Loss: 0.85485768\n","Epoch [37/50], Step [30/164], Loss: 0.74611032\n","Epoch [37/50], Step [45/164], Loss: 0.84444112\n","Epoch [37/50], Step [60/164], Loss: 0.73443502\n","Epoch [37/50], Step [75/164], Loss: 0.75536984\n","Epoch [37/50], Step [90/164], Loss: 0.78133583\n","Epoch [37/50], Step [105/164], Loss: 0.81592613\n","Epoch [37/50], Step [120/164], Loss: 0.86259621\n","Epoch [37/50], Step [135/164], Loss: 0.75920314\n","Epoch [37/50], Step [150/164], Loss: 0.78474849\n","Evaluating model in epoch 37 Val loss/acc: 0.99744700|72.5541 Test loss/acc: 0.99143647|72.3362\n","Epoch [38/50], Step [15/164], Loss: 0.82762969\n","Epoch [38/50], Step [30/164], Loss: 0.76943862\n","Epoch [38/50], Step [45/164], Loss: 0.82084721\n","Epoch [38/50], Step [60/164], Loss: 0.70332420\n","Epoch [38/50], Step [75/164], Loss: 0.78979927\n","Epoch [38/50], Step [90/164], Loss: 0.67607182\n","Epoch [38/50], Step [105/164], Loss: 0.74830180\n","Epoch [38/50], Step [120/164], Loss: 0.65719169\n","Epoch [38/50], Step [135/164], Loss: 0.71078467\n","Epoch [38/50], Step [150/164], Loss: 0.86769998\n","Evaluating model in epoch 38 Val loss/acc: 0.99559797|72.6923 Test loss/acc: 0.99878819|72.3862\n","Epoch [39/50], Step [15/164], Loss: 0.88701648\n","Epoch [39/50], Step [30/164], Loss: 0.72735333\n","Epoch [39/50], Step [45/164], Loss: 0.86232692\n","Epoch [39/50], Step [60/164], Loss: 0.94947964\n","Epoch [39/50], Step [75/164], Loss: 0.63677931\n","Epoch [39/50], Step [90/164], Loss: 0.82914722\n","Epoch [39/50], Step [105/164], Loss: 0.71613002\n","Epoch [39/50], Step [120/164], Loss: 0.75945342\n","Epoch [39/50], Step [135/164], Loss: 0.61657208\n","Epoch [39/50], Step [150/164], Loss: 0.79571897\n","Evaluating model in epoch 39 Val loss/acc: 0.99741337|72.8009 Test loss/acc: 1.00546460|72.4362\n","Epoch [40/50], Step [15/164], Loss: 0.77955246\n","Epoch [40/50], Step [30/164], Loss: 0.77565426\n","Epoch [40/50], Step [45/164], Loss: 0.75346607\n","Epoch [40/50], Step [60/164], Loss: 0.66363192\n","Epoch [40/50], Step [75/164], Loss: 0.76280874\n","Epoch [40/50], Step [90/164], Loss: 0.77299732\n","Epoch [40/50], Step [105/164], Loss: 0.79966956\n","Epoch [40/50], Step [120/164], Loss: 0.70150793\n","Epoch [40/50], Step [135/164], Loss: 0.78875768\n","Epoch [40/50], Step [150/164], Loss: 0.82656914\n","Evaluating model in epoch 40 Val loss/acc: 0.99999910|72.8107 Test loss/acc: 1.00354894|72.5863\n","Epoch [41/50], Step [15/164], Loss: 0.91787773\n","Epoch [41/50], Step [30/164], Loss: 0.68464291\n","Epoch [41/50], Step [45/164], Loss: 0.62268966\n"]}]},{"cell_type":"markdown","metadata":{"id":"Clti4s5YAkRk"},"source":["## STEP 2: Create a multi head model, train and evaluate\n","\n","Go and complete the `MultiHeadSelfAttention` class. Then, train and evaluate the model using 4 heads.\n"," \n"," You should obtain a test accuracy of\n"," - ~31.0% with 1 epoch\n"," - ~43.6% with 2 epochs\n"," - ~71.0% with 20 epochs\n"," - ~73.2% with 50 epochs\n","\n","Compute only the first two epochs during class hours (you can stop the execution afterwards), as it takes almost an hour to train the model for 50 epochs and you should see if the network is learning correctly the classification task by the end of the second epoch.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhS0CMvuAnSj","executionInfo":{"status":"ok","timestamp":1636473878062,"user_tz":-60,"elapsed":3122414,"user":{"displayName":"Ander Salaberria","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11000269261626730640"}},"outputId":"628d0115-fdc3-44b9-ebc3-23831d0a40d8"},"source":["model=PLClassifier(NUM_LAYERS, HIDDEN_DIM, 4, tok.get_vocab_size(), N_LABELS)\n","model=model.to(device)\n","\n","logdir = \"logs/multihead_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","writer = SummaryWriter(logdir)\n","\n","train(model, NUM_EPOCHS, train_loader, val_loader, test_loader, writer)\n","writer.flush()\n","\n","# evaluate the model\n","test_loss, test_acc = evaluate(model, test_loader)\n","print(f'Test loss/accuracy: {test_loss:.4f}/{test_acc:.4f}')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/50], Step [15/164], Loss: 3.06091404\n","Epoch [1/50], Step [30/164], Loss: 2.98493552\n","Epoch [1/50], Step [45/164], Loss: 2.94785380\n","Epoch [1/50], Step [60/164], Loss: 2.92106366\n","Epoch [1/50], Step [75/164], Loss: 2.81041551\n","Epoch [1/50], Step [90/164], Loss: 2.70765924\n","Epoch [1/50], Step [105/164], Loss: 2.58929300\n","Epoch [1/50], Step [120/164], Loss: 2.50426817\n","Epoch [1/50], Step [135/164], Loss: 2.40064144\n","Epoch [1/50], Step [150/164], Loss: 2.35465741\n","Evaluating model in epoch 1 Val loss/acc: 2.22974254|36.2326 Test loss/acc: 2.23652828|35.3177\n","Epoch [2/50], Step [15/164], Loss: 2.31760406\n","Epoch [2/50], Step [30/164], Loss: 2.21484375\n","Epoch [2/50], Step [45/164], Loss: 2.21931481\n","Epoch [2/50], Step [60/164], Loss: 2.12833667\n","Epoch [2/50], Step [75/164], Loss: 2.30586791\n","Epoch [2/50], Step [90/164], Loss: 2.09697914\n","Epoch [2/50], Step [105/164], Loss: 2.20498276\n","Epoch [2/50], Step [120/164], Loss: 2.12840843\n","Epoch [2/50], Step [135/164], Loss: 2.05285311\n","Epoch [2/50], Step [150/164], Loss: 2.05121207\n","Evaluating model in epoch 2 Val loss/acc: 1.85049823|47.6651 Test loss/acc: 1.86528105|46.4232\n","Epoch [3/50], Step [15/164], Loss: 1.95827496\n","Epoch [3/50], Step [30/164], Loss: 1.89517939\n","Epoch [3/50], Step [45/164], Loss: 1.92505288\n","Epoch [3/50], Step [60/164], Loss: 1.92949378\n","Epoch [3/50], Step [75/164], Loss: 1.82093787\n","Epoch [3/50], Step [90/164], Loss: 1.80654597\n","Epoch [3/50], Step [105/164], Loss: 1.71995568\n","Epoch [3/50], Step [120/164], Loss: 1.83456838\n","Epoch [3/50], Step [135/164], Loss: 1.87159550\n","Epoch [3/50], Step [150/164], Loss: 1.75355554\n","Evaluating model in epoch 3 Val loss/acc: 1.60053503|54.9807 Test loss/acc: 1.62591849|54.3772\n","Epoch [4/50], Step [15/164], Loss: 1.65646660\n","Epoch [4/50], Step [30/164], Loss: 1.47883642\n","Epoch [4/50], Step [45/164], Loss: 1.68134463\n","Epoch [4/50], Step [60/164], Loss: 1.56186330\n","Epoch [4/50], Step [75/164], Loss: 1.68988180\n","Epoch [4/50], Step [90/164], Loss: 1.49654877\n","Epoch [4/50], Step [105/164], Loss: 1.72936344\n","Epoch [4/50], Step [120/164], Loss: 1.63687134\n","Epoch [4/50], Step [135/164], Loss: 1.39114738\n","Epoch [4/50], Step [150/164], Loss: 1.59067845\n","Evaluating model in epoch 4 Val loss/acc: 1.45428183|58.4164 Test loss/acc: 1.48300286|57.8289\n","Epoch [5/50], Step [15/164], Loss: 1.52318656\n","Epoch [5/50], Step [30/164], Loss: 1.42596221\n","Epoch [5/50], Step [45/164], Loss: 1.49480164\n","Epoch [5/50], Step [60/164], Loss: 1.51800168\n","Epoch [5/50], Step [75/164], Loss: 1.48437905\n","Epoch [5/50], Step [90/164], Loss: 1.50034022\n","Epoch [5/50], Step [105/164], Loss: 1.45572686\n","Epoch [5/50], Step [120/164], Loss: 1.53231609\n","Epoch [5/50], Step [135/164], Loss: 1.56247091\n","Epoch [5/50], Step [150/164], Loss: 1.44631934\n","Evaluating model in epoch 5 Val loss/acc: 1.35234418|61.1314 Test loss/acc: 1.38119023|59.5298\n","Epoch [6/50], Step [15/164], Loss: 1.56549823\n","Epoch [6/50], Step [30/164], Loss: 1.43768191\n","Epoch [6/50], Step [45/164], Loss: 1.38763702\n","Epoch [6/50], Step [60/164], Loss: 1.25823963\n","Epoch [6/50], Step [75/164], Loss: 1.35242617\n","Epoch [6/50], Step [90/164], Loss: 1.38693309\n","Epoch [6/50], Step [105/164], Loss: 1.22787666\n","Epoch [6/50], Step [120/164], Loss: 1.38357246\n","Epoch [6/50], Step [135/164], Loss: 1.33615232\n","Epoch [6/50], Step [150/164], Loss: 1.22595644\n","Evaluating model in epoch 6 Val loss/acc: 1.27649463|63.0664 Test loss/acc: 1.31694040|61.7309\n","Epoch [7/50], Step [15/164], Loss: 1.47355008\n","Epoch [7/50], Step [30/164], Loss: 1.32830000\n","Epoch [7/50], Step [45/164], Loss: 1.34153676\n","Epoch [7/50], Step [60/164], Loss: 1.29202473\n","Epoch [7/50], Step [75/164], Loss: 1.17449558\n","Epoch [7/50], Step [90/164], Loss: 1.39727330\n","Epoch [7/50], Step [105/164], Loss: 1.24058568\n","Epoch [7/50], Step [120/164], Loss: 1.24971414\n","Epoch [7/50], Step [135/164], Loss: 1.50675368\n","Epoch [7/50], Step [150/164], Loss: 1.21401465\n","Evaluating model in epoch 7 Val loss/acc: 1.21976790|64.8830 Test loss/acc: 1.25006770|63.5818\n","Epoch [8/50], Step [15/164], Loss: 1.15902352\n","Epoch [8/50], Step [30/164], Loss: 1.23219478\n","Epoch [8/50], Step [45/164], Loss: 1.25713515\n","Epoch [8/50], Step [60/164], Loss: 1.36142385\n","Epoch [8/50], Step [75/164], Loss: 1.13924015\n","Epoch [8/50], Step [90/164], Loss: 1.16583407\n","Epoch [8/50], Step [105/164], Loss: 1.35323751\n","Epoch [8/50], Step [120/164], Loss: 1.26585662\n","Epoch [8/50], Step [135/164], Loss: 1.24624324\n","Epoch [8/50], Step [150/164], Loss: 1.14681423\n","Evaluating model in epoch 8 Val loss/acc: 1.17139807|66.4922 Test loss/acc: 1.20452128|65.1826\n","Epoch [9/50], Step [15/164], Loss: 1.09295571\n","Epoch [9/50], Step [30/164], Loss: 1.12899768\n","Epoch [9/50], Step [45/164], Loss: 1.16539598\n","Epoch [9/50], Step [60/164], Loss: 1.09913599\n","Epoch [9/50], Step [75/164], Loss: 1.30588150\n","Epoch [9/50], Step [90/164], Loss: 1.17452979\n","Epoch [9/50], Step [105/164], Loss: 1.16187024\n","Epoch [9/50], Step [120/164], Loss: 1.10264421\n","Epoch [9/50], Step [135/164], Loss: 1.10335946\n","Epoch [9/50], Step [150/164], Loss: 1.18903875\n","Evaluating model in epoch 9 Val loss/acc: 1.13839837|67.4598 Test loss/acc: 1.16773015|66.5833\n","Epoch [10/50], Step [15/164], Loss: 1.29934299\n","Epoch [10/50], Step [30/164], Loss: 1.17580938\n","Epoch [10/50], Step [45/164], Loss: 1.12993085\n","Epoch [10/50], Step [60/164], Loss: 1.12602878\n","Epoch [10/50], Step [75/164], Loss: 1.12420738\n","Epoch [10/50], Step [90/164], Loss: 1.21021700\n","Epoch [10/50], Step [105/164], Loss: 1.25591028\n","Epoch [10/50], Step [120/164], Loss: 1.14154494\n","Epoch [10/50], Step [135/164], Loss: 1.12873375\n","Epoch [10/50], Step [150/164], Loss: 1.07512426\n","Evaluating model in epoch 10 Val loss/acc: 1.11590186|68.2200 Test loss/acc: 1.14115289|67.3337\n","Epoch [11/50], Step [15/164], Loss: 1.08343363\n","Epoch [11/50], Step [30/164], Loss: 1.05063868\n","Epoch [11/50], Step [45/164], Loss: 1.07626748\n","Epoch [11/50], Step [60/164], Loss: 1.04055154\n","Epoch [11/50], Step [75/164], Loss: 1.04663229\n","Epoch [11/50], Step [90/164], Loss: 1.14471400\n","Epoch [11/50], Step [105/164], Loss: 1.20233476\n","Epoch [11/50], Step [120/164], Loss: 1.07304740\n","Epoch [11/50], Step [135/164], Loss: 1.24135888\n","Epoch [11/50], Step [150/164], Loss: 1.16308630\n","Evaluating model in epoch 11 Val loss/acc: 1.10107434|68.8222 Test loss/acc: 1.12783918|67.6338\n","Epoch [12/50], Step [15/164], Loss: 1.15158212\n","Epoch [12/50], Step [30/164], Loss: 1.11816251\n","Epoch [12/50], Step [45/164], Loss: 1.02990139\n","Epoch [12/50], Step [60/164], Loss: 1.10205746\n","Epoch [12/50], Step [75/164], Loss: 1.02050745\n","Epoch [12/50], Step [90/164], Loss: 1.12602973\n","Epoch [12/50], Step [105/164], Loss: 1.23284996\n","Epoch [12/50], Step [120/164], Loss: 1.10497856\n","Epoch [12/50], Step [135/164], Loss: 1.03653169\n","Epoch [12/50], Step [150/164], Loss: 1.07171500\n","Evaluating model in epoch 12 Val loss/acc: 1.07385329|69.4442 Test loss/acc: 1.10041568|69.5348\n","Epoch [13/50], Step [15/164], Loss: 1.06561947\n","Epoch [13/50], Step [30/164], Loss: 1.16979027\n","Epoch [13/50], Step [45/164], Loss: 0.98422003\n","Epoch [13/50], Step [60/164], Loss: 1.00873327\n","Epoch [13/50], Step [75/164], Loss: 0.96231902\n","Epoch [13/50], Step [90/164], Loss: 1.20669174\n","Epoch [13/50], Step [105/164], Loss: 1.09045196\n","Epoch [13/50], Step [120/164], Loss: 1.12303138\n","Epoch [13/50], Step [135/164], Loss: 1.21011698\n","Epoch [13/50], Step [150/164], Loss: 0.93125314\n","Evaluating model in epoch 13 Val loss/acc: 1.06248079|70.0859 Test loss/acc: 1.08679108|69.9350\n","Epoch [14/50], Step [15/164], Loss: 1.12382162\n","Epoch [14/50], Step [30/164], Loss: 0.89248562\n","Epoch [14/50], Step [45/164], Loss: 0.94753772\n","Epoch [14/50], Step [60/164], Loss: 1.15423119\n","Epoch [14/50], Step [75/164], Loss: 1.11487019\n","Epoch [14/50], Step [90/164], Loss: 0.92615527\n","Epoch [14/50], Step [105/164], Loss: 1.05344558\n","Epoch [14/50], Step [120/164], Loss: 1.05955732\n","Epoch [14/50], Step [135/164], Loss: 1.00334680\n","Epoch [14/50], Step [150/164], Loss: 1.00894189\n","Evaluating model in epoch 14 Val loss/acc: 1.04438058|70.5795 Test loss/acc: 1.07395776|70.2351\n","Epoch [15/50], Step [15/164], Loss: 1.13351035\n","Epoch [15/50], Step [30/164], Loss: 1.10293245\n","Epoch [15/50], Step [45/164], Loss: 0.99813366\n","Epoch [15/50], Step [60/164], Loss: 1.18516290\n","Epoch [15/50], Step [75/164], Loss: 0.94990629\n","Epoch [15/50], Step [90/164], Loss: 1.08844578\n","Epoch [15/50], Step [105/164], Loss: 0.97864097\n","Epoch [15/50], Step [120/164], Loss: 1.00188947\n","Epoch [15/50], Step [135/164], Loss: 0.90055096\n","Epoch [15/50], Step [150/164], Loss: 0.94721752\n","Evaluating model in epoch 15 Val loss/acc: 1.03453907|70.9053 Test loss/acc: 1.06364053|70.1351\n","Epoch [16/50], Step [15/164], Loss: 0.97361010\n","Epoch [16/50], Step [30/164], Loss: 0.92974019\n","Epoch [16/50], Step [45/164], Loss: 0.98693359\n","Epoch [16/50], Step [60/164], Loss: 0.94500798\n","Epoch [16/50], Step [75/164], Loss: 1.15637839\n","Epoch [16/50], Step [90/164], Loss: 0.90155584\n","Epoch [16/50], Step [105/164], Loss: 1.13615048\n","Epoch [16/50], Step [120/164], Loss: 1.02545226\n","Epoch [16/50], Step [135/164], Loss: 1.02935827\n","Epoch [16/50], Step [150/164], Loss: 0.95919716\n","Evaluating model in epoch 16 Val loss/acc: 1.02492623|70.9744 Test loss/acc: 1.05753925|71.0355\n","Epoch [17/50], Step [15/164], Loss: 1.04215360\n","Epoch [17/50], Step [30/164], Loss: 1.16921508\n","Epoch [17/50], Step [45/164], Loss: 0.92530984\n","Epoch [17/50], Step [60/164], Loss: 0.94922382\n","Epoch [17/50], Step [75/164], Loss: 0.83595806\n","Epoch [17/50], Step [90/164], Loss: 0.96780086\n","Epoch [17/50], Step [105/164], Loss: 1.10703576\n","Epoch [17/50], Step [120/164], Loss: 0.97392869\n","Epoch [17/50], Step [135/164], Loss: 0.93449378\n","Epoch [17/50], Step [150/164], Loss: 0.93212521\n","Evaluating model in epoch 17 Val loss/acc: 1.01185334|71.5273 Test loss/acc: 1.03708117|70.3352\n","Epoch [18/50], Step [15/164], Loss: 1.01089883\n","Epoch [18/50], Step [30/164], Loss: 0.91628748\n","Epoch [18/50], Step [45/164], Loss: 1.07504237\n","Epoch [18/50], Step [60/164], Loss: 1.03192520\n","Epoch [18/50], Step [75/164], Loss: 1.02728415\n","Epoch [18/50], Step [90/164], Loss: 1.05163050\n","Epoch [18/50], Step [105/164], Loss: 0.96233362\n","Epoch [18/50], Step [120/164], Loss: 1.00768018\n","Epoch [18/50], Step [135/164], Loss: 0.94592768\n","Epoch [18/50], Step [150/164], Loss: 0.82795626\n","Evaluating model in epoch 18 Val loss/acc: 1.00697660|71.4286 Test loss/acc: 1.04145091|71.0855\n","Epoch [19/50], Step [15/164], Loss: 1.02319562\n","Epoch [19/50], Step [30/164], Loss: 0.78300828\n","Epoch [19/50], Step [45/164], Loss: 1.08930755\n","Epoch [19/50], Step [60/164], Loss: 0.92126429\n","Epoch [19/50], Step [75/164], Loss: 0.91308147\n","Epoch [19/50], Step [90/164], Loss: 0.98149872\n","Epoch [19/50], Step [105/164], Loss: 0.98140502\n","Epoch [19/50], Step [120/164], Loss: 0.91471553\n","Epoch [19/50], Step [135/164], Loss: 0.94334304\n","Epoch [19/50], Step [150/164], Loss: 0.96630961\n","Evaluating model in epoch 19 Val loss/acc: 0.99767802|71.8531 Test loss/acc: 1.02901056|71.8859\n","Epoch [20/50], Step [15/164], Loss: 0.99700153\n","Epoch [20/50], Step [30/164], Loss: 0.89606100\n","Epoch [20/50], Step [45/164], Loss: 1.04106641\n","Epoch [20/50], Step [60/164], Loss: 0.91802824\n","Epoch [20/50], Step [75/164], Loss: 0.93252766\n","Epoch [20/50], Step [90/164], Loss: 1.07145333\n","Epoch [20/50], Step [105/164], Loss: 0.87943465\n","Epoch [20/50], Step [120/164], Loss: 0.96638340\n","Epoch [20/50], Step [135/164], Loss: 0.91563988\n","Epoch [20/50], Step [150/164], Loss: 0.87787813\n","Evaluating model in epoch 20 Val loss/acc: 0.99439675|72.0209 Test loss/acc: 1.02479196|71.3857\n","Epoch [21/50], Step [15/164], Loss: 0.87353826\n","Epoch [21/50], Step [30/164], Loss: 1.01163721\n","Epoch [21/50], Step [45/164], Loss: 0.89127797\n","Epoch [21/50], Step [60/164], Loss: 0.91603190\n","Epoch [21/50], Step [75/164], Loss: 0.95861369\n","Epoch [21/50], Step [90/164], Loss: 0.93277305\n","Epoch [21/50], Step [105/164], Loss: 0.68906546\n","Epoch [21/50], Step [120/164], Loss: 0.86810702\n","Epoch [21/50], Step [135/164], Loss: 0.85141587\n","Epoch [21/50], Step [150/164], Loss: 0.84109163\n","Evaluating model in epoch 21 Val loss/acc: 0.99661331|72.0999 Test loss/acc: 1.02276034|71.6858\n","Epoch [22/50], Step [15/164], Loss: 0.87281811\n","Epoch [22/50], Step [30/164], Loss: 0.81970477\n","Epoch [22/50], Step [45/164], Loss: 0.71831226\n","Epoch [22/50], Step [60/164], Loss: 0.88215762\n","Epoch [22/50], Step [75/164], Loss: 1.00126326\n","Epoch [22/50], Step [90/164], Loss: 0.89521652\n","Epoch [22/50], Step [105/164], Loss: 1.05204666\n","Epoch [22/50], Step [120/164], Loss: 1.01116180\n","Epoch [22/50], Step [135/164], Loss: 0.83678925\n","Epoch [22/50], Step [150/164], Loss: 0.87590706\n","Evaluating model in epoch 22 Val loss/acc: 0.98461029|72.4751 Test loss/acc: 1.00989795|71.9360\n","Epoch [23/50], Step [15/164], Loss: 0.96435714\n","Epoch [23/50], Step [30/164], Loss: 0.92885613\n","Epoch [23/50], Step [45/164], Loss: 1.01958454\n","Epoch [23/50], Step [60/164], Loss: 0.93214279\n","Epoch [23/50], Step [75/164], Loss: 0.82980770\n","Epoch [23/50], Step [90/164], Loss: 0.75293851\n","Epoch [23/50], Step [105/164], Loss: 0.89532870\n","Epoch [23/50], Step [120/164], Loss: 0.88906437\n","Epoch [23/50], Step [135/164], Loss: 0.81103975\n","Epoch [23/50], Step [150/164], Loss: 0.86624891\n","Evaluating model in epoch 23 Val loss/acc: 0.98583783|72.4060 Test loss/acc: 1.01389418|71.7859\n","Epoch [24/50], Step [15/164], Loss: 0.93033046\n","Epoch [24/50], Step [30/164], Loss: 0.86681640\n","Epoch [24/50], Step [45/164], Loss: 0.78501910\n","Epoch [24/50], Step [60/164], Loss: 0.88557833\n","Epoch [24/50], Step [75/164], Loss: 0.86477309\n","Epoch [24/50], Step [90/164], Loss: 0.87483072\n","Epoch [24/50], Step [105/164], Loss: 0.97034782\n","Epoch [24/50], Step [120/164], Loss: 0.92361826\n","Epoch [24/50], Step [135/164], Loss: 0.86165977\n","Epoch [24/50], Step [150/164], Loss: 0.98875684\n","Evaluating model in epoch 24 Val loss/acc: 0.98150894|72.5244 Test loss/acc: 1.00699089|72.4362\n","Epoch [25/50], Step [15/164], Loss: 0.86564171\n","Epoch [25/50], Step [30/164], Loss: 0.77584457\n","Epoch [25/50], Step [45/164], Loss: 0.83099568\n","Epoch [25/50], Step [60/164], Loss: 0.87864214\n","Epoch [25/50], Step [75/164], Loss: 0.90334773\n","Epoch [25/50], Step [90/164], Loss: 0.81114399\n","Epoch [25/50], Step [105/164], Loss: 0.78763884\n","Epoch [25/50], Step [120/164], Loss: 0.76147294\n","Epoch [25/50], Step [135/164], Loss: 1.06662893\n","Epoch [25/50], Step [150/164], Loss: 0.89378440\n","Evaluating model in epoch 25 Val loss/acc: 0.97458386|72.8009 Test loss/acc: 1.00025012|72.3362\n","Epoch [26/50], Step [15/164], Loss: 0.74428761\n","Epoch [26/50], Step [30/164], Loss: 0.78796679\n","Epoch [26/50], Step [45/164], Loss: 0.78368676\n","Epoch [26/50], Step [60/164], Loss: 0.79828459\n","Epoch [26/50], Step [75/164], Loss: 0.89651650\n","Epoch [26/50], Step [90/164], Loss: 0.92959321\n","Epoch [26/50], Step [105/164], Loss: 0.96346420\n","Epoch [26/50], Step [120/164], Loss: 0.90569168\n","Epoch [26/50], Step [135/164], Loss: 0.87935507\n","Epoch [26/50], Step [150/164], Loss: 0.91486007\n","Evaluating model in epoch 26 Val loss/acc: 0.97318686|72.9588 Test loss/acc: 1.00328469|72.0860\n","Epoch [27/50], Step [15/164], Loss: 0.83663774\n","Epoch [27/50], Step [30/164], Loss: 0.98056662\n","Epoch [27/50], Step [45/164], Loss: 0.99027789\n","Epoch [27/50], Step [60/164], Loss: 0.92814851\n","Epoch [27/50], Step [75/164], Loss: 0.83648479\n","Epoch [27/50], Step [90/164], Loss: 0.72250855\n","Epoch [27/50], Step [105/164], Loss: 0.70722717\n","Epoch [27/50], Step [120/164], Loss: 0.93464255\n","Epoch [27/50], Step [135/164], Loss: 0.79467541\n","Epoch [27/50], Step [150/164], Loss: 0.81073588\n","Evaluating model in epoch 27 Val loss/acc: 0.97866164|72.7614 Test loss/acc: 1.01174342|71.8859\n","Epoch [28/50], Step [15/164], Loss: 0.81329513\n","Epoch [28/50], Step [30/164], Loss: 0.77985662\n","Epoch [28/50], Step [45/164], Loss: 0.79454648\n","Epoch [28/50], Step [60/164], Loss: 0.80841321\n","Epoch [28/50], Step [75/164], Loss: 0.85709798\n","Epoch [28/50], Step [90/164], Loss: 0.90034699\n","Epoch [28/50], Step [105/164], Loss: 0.86564595\n","Epoch [28/50], Step [120/164], Loss: 0.80591327\n","Epoch [28/50], Step [135/164], Loss: 0.91255659\n","Epoch [28/50], Step [150/164], Loss: 0.77317870\n","Evaluating model in epoch 28 Val loss/acc: 0.97306761|73.0674 Test loss/acc: 1.00642405|72.4362\n","Epoch [29/50], Step [15/164], Loss: 0.83269906\n","Epoch [29/50], Step [30/164], Loss: 0.79133737\n","Epoch [29/50], Step [45/164], Loss: 0.77347946\n","Epoch [29/50], Step [60/164], Loss: 0.75152677\n","Epoch [29/50], Step [75/164], Loss: 0.83218879\n","Epoch [29/50], Step [90/164], Loss: 0.76253563\n","Epoch [29/50], Step [105/164], Loss: 0.68696898\n","Epoch [29/50], Step [120/164], Loss: 0.69978487\n","Epoch [29/50], Step [135/164], Loss: 0.78578222\n","Epoch [29/50], Step [150/164], Loss: 0.83081639\n","Evaluating model in epoch 29 Val loss/acc: 0.97592268|72.8798 Test loss/acc: 1.01386534|72.2361\n","Epoch [30/50], Step [15/164], Loss: 0.85793662\n","Epoch [30/50], Step [30/164], Loss: 0.77963299\n","Epoch [30/50], Step [45/164], Loss: 0.69797647\n","Epoch [30/50], Step [60/164], Loss: 0.87408966\n","Epoch [30/50], Step [75/164], Loss: 0.92650682\n","Epoch [30/50], Step [90/164], Loss: 0.88858998\n","Epoch [30/50], Step [105/164], Loss: 0.81107622\n","Epoch [30/50], Step [120/164], Loss: 0.79903263\n","Epoch [30/50], Step [135/164], Loss: 0.79195440\n","Epoch [30/50], Step [150/164], Loss: 0.82515174\n","Evaluating model in epoch 30 Val loss/acc: 0.97640280|73.0773 Test loss/acc: 1.00583116|72.7364\n","Epoch [31/50], Step [15/164], Loss: 0.76689386\n","Epoch [31/50], Step [30/164], Loss: 0.76886922\n","Epoch [31/50], Step [45/164], Loss: 0.80408359\n","Epoch [31/50], Step [60/164], Loss: 0.81982511\n","Epoch [31/50], Step [75/164], Loss: 0.67639768\n","Epoch [31/50], Step [90/164], Loss: 0.87040126\n","Epoch [31/50], Step [105/164], Loss: 0.74473143\n","Epoch [31/50], Step [120/164], Loss: 0.89031303\n","Epoch [31/50], Step [135/164], Loss: 0.82186127\n","Epoch [31/50], Step [150/164], Loss: 0.70643508\n","Evaluating model in epoch 31 Val loss/acc: 0.96982931|73.4821 Test loss/acc: 0.99848692|72.5863\n","Epoch [32/50], Step [15/164], Loss: 0.86758697\n","Epoch [32/50], Step [30/164], Loss: 0.67085254\n","Epoch [32/50], Step [45/164], Loss: 0.79947013\n","Epoch [32/50], Step [60/164], Loss: 0.73664135\n","Epoch [32/50], Step [75/164], Loss: 0.77555585\n","Epoch [32/50], Step [90/164], Loss: 0.85307729\n","Epoch [32/50], Step [105/164], Loss: 0.78102070\n","Epoch [32/50], Step [120/164], Loss: 0.73797041\n","Epoch [32/50], Step [135/164], Loss: 0.88357723\n","Epoch [32/50], Step [150/164], Loss: 0.75948775\n","Evaluating model in epoch 32 Val loss/acc: 0.96435149|73.2748 Test loss/acc: 0.99225152|73.0865\n","Epoch [33/50], Step [15/164], Loss: 0.64747769\n","Epoch [33/50], Step [30/164], Loss: 0.81009561\n","Epoch [33/50], Step [45/164], Loss: 0.71334249\n","Epoch [33/50], Step [60/164], Loss: 0.72373641\n","Epoch [33/50], Step [75/164], Loss: 0.88359696\n","Epoch [33/50], Step [90/164], Loss: 0.69098121\n","Epoch [33/50], Step [105/164], Loss: 0.88605320\n","Epoch [33/50], Step [120/164], Loss: 0.68514121\n","Epoch [33/50], Step [135/164], Loss: 0.96093357\n","Epoch [33/50], Step [150/164], Loss: 0.81129342\n","Evaluating model in epoch 33 Val loss/acc: 0.96631752|73.1464 Test loss/acc: 0.99738591|72.0360\n","Epoch [34/50], Step [15/164], Loss: 0.73711944\n","Epoch [34/50], Step [30/164], Loss: 0.68671471\n","Epoch [34/50], Step [45/164], Loss: 0.87044090\n","Epoch [34/50], Step [60/164], Loss: 0.92484051\n","Epoch [34/50], Step [75/164], Loss: 0.80007446\n","Epoch [34/50], Step [90/164], Loss: 0.75170451\n","Epoch [34/50], Step [105/164], Loss: 0.73374420\n","Epoch [34/50], Step [120/164], Loss: 0.80527592\n","Epoch [34/50], Step [135/164], Loss: 0.77618927\n","Epoch [34/50], Step [150/164], Loss: 0.88034314\n","Evaluating model in epoch 34 Val loss/acc: 0.96190473|73.3241 Test loss/acc: 0.98759475|72.5863\n","Epoch [35/50], Step [15/164], Loss: 0.72789156\n","Epoch [35/50], Step [30/164], Loss: 0.74271476\n","Epoch [35/50], Step [45/164], Loss: 0.79995441\n","Epoch [35/50], Step [60/164], Loss: 0.75144631\n","Epoch [35/50], Step [75/164], Loss: 0.70871216\n","Epoch [35/50], Step [90/164], Loss: 0.75762546\n","Epoch [35/50], Step [105/164], Loss: 0.81215465\n","Epoch [35/50], Step [120/164], Loss: 0.83740741\n","Epoch [35/50], Step [135/164], Loss: 0.64594555\n","Epoch [35/50], Step [150/164], Loss: 0.72014290\n","Evaluating model in epoch 35 Val loss/acc: 0.96390322|73.5216 Test loss/acc: 1.00081695|72.3862\n","Epoch [36/50], Step [15/164], Loss: 0.66057926\n","Epoch [36/50], Step [30/164], Loss: 0.72387338\n","Epoch [36/50], Step [45/164], Loss: 0.69853109\n","Epoch [36/50], Step [60/164], Loss: 0.61530423\n","Epoch [36/50], Step [75/164], Loss: 0.81628418\n","Epoch [36/50], Step [90/164], Loss: 0.70798057\n","Epoch [36/50], Step [105/164], Loss: 0.86708540\n","Epoch [36/50], Step [120/164], Loss: 0.68838143\n","Epoch [36/50], Step [135/164], Loss: 0.79513776\n","Epoch [36/50], Step [150/164], Loss: 0.90025610\n","Evaluating model in epoch 36 Val loss/acc: 0.96895582|73.1958 Test loss/acc: 1.00367677|72.3362\n","Epoch [37/50], Step [15/164], Loss: 0.70698386\n","Epoch [37/50], Step [30/164], Loss: 0.80337793\n","Epoch [37/50], Step [45/164], Loss: 0.68736076\n","Epoch [37/50], Step [60/164], Loss: 0.75545651\n","Epoch [37/50], Step [75/164], Loss: 0.80504560\n","Epoch [37/50], Step [90/164], Loss: 0.81281424\n","Epoch [37/50], Step [105/164], Loss: 0.61097544\n","Epoch [37/50], Step [120/164], Loss: 0.75945204\n","Epoch [37/50], Step [135/164], Loss: 0.80956751\n","Epoch [37/50], Step [150/164], Loss: 0.70219684\n","Evaluating model in epoch 37 Val loss/acc: 0.96947712|73.6104 Test loss/acc: 1.00556042|72.9365\n","Epoch [38/50], Step [15/164], Loss: 0.77581829\n","Epoch [38/50], Step [30/164], Loss: 0.69434237\n","Epoch [38/50], Step [45/164], Loss: 0.70758140\n","Epoch [38/50], Step [60/164], Loss: 0.64989597\n","Epoch [38/50], Step [75/164], Loss: 0.76121849\n","Epoch [38/50], Step [90/164], Loss: 0.71637172\n","Epoch [38/50], Step [105/164], Loss: 0.86580092\n","Epoch [38/50], Step [120/164], Loss: 0.76903367\n","Epoch [38/50], Step [135/164], Loss: 0.73512208\n","Epoch [38/50], Step [150/164], Loss: 0.79127109\n","Evaluating model in epoch 38 Val loss/acc: 0.97219357|73.3834 Test loss/acc: 1.00468828|72.5363\n","Epoch [39/50], Step [15/164], Loss: 0.56223923\n","Epoch [39/50], Step [30/164], Loss: 0.65125686\n","Epoch [39/50], Step [45/164], Loss: 0.75096226\n","Epoch [39/50], Step [60/164], Loss: 0.71293759\n","Epoch [39/50], Step [75/164], Loss: 0.62123388\n","Epoch [39/50], Step [90/164], Loss: 0.72711909\n","Epoch [39/50], Step [105/164], Loss: 0.70894819\n","Epoch [39/50], Step [120/164], Loss: 0.89057237\n","Epoch [39/50], Step [135/164], Loss: 0.60865700\n","Epoch [39/50], Step [150/164], Loss: 0.71240729\n","Evaluating model in epoch 39 Val loss/acc: 0.96977919|73.4821 Test loss/acc: 1.00750152|73.3867\n","Epoch [40/50], Step [15/164], Loss: 0.62165695\n","Epoch [40/50], Step [30/164], Loss: 0.76728857\n","Epoch [40/50], Step [45/164], Loss: 0.78405207\n","Epoch [40/50], Step [60/164], Loss: 0.65634835\n","Epoch [40/50], Step [75/164], Loss: 0.61152339\n","Epoch [40/50], Step [90/164], Loss: 0.72014737\n","Epoch [40/50], Step [105/164], Loss: 0.70768261\n","Epoch [40/50], Step [120/164], Loss: 0.72731453\n","Epoch [40/50], Step [135/164], Loss: 0.76752579\n","Epoch [40/50], Step [150/164], Loss: 0.74997675\n","Evaluating model in epoch 40 Val loss/acc: 0.96916882|73.5314 Test loss/acc: 1.00849584|72.8364\n","Epoch [41/50], Step [15/164], Loss: 0.81974185\n","Epoch [41/50], Step [30/164], Loss: 0.66065913\n","Epoch [41/50], Step [45/164], Loss: 0.93420017\n","Epoch [41/50], Step [60/164], Loss: 0.66625082\n","Epoch [41/50], Step [75/164], Loss: 0.68880242\n","Epoch [41/50], Step [90/164], Loss: 0.70195639\n","Epoch [41/50], Step [105/164], Loss: 0.65589672\n","Epoch [41/50], Step [120/164], Loss: 0.76761359\n","Epoch [41/50], Step [135/164], Loss: 0.68691725\n","Epoch [41/50], Step [150/164], Loss: 0.72543174\n","Evaluating model in epoch 41 Val loss/acc: 0.97229785|73.5314 Test loss/acc: 1.01518829|72.2861\n","Epoch [42/50], Step [15/164], Loss: 0.64850760\n","Epoch [42/50], Step [30/164], Loss: 0.62912494\n","Epoch [42/50], Step [45/164], Loss: 0.77280962\n","Epoch [42/50], Step [60/164], Loss: 0.72394830\n","Epoch [42/50], Step [75/164], Loss: 0.59678990\n","Epoch [42/50], Step [90/164], Loss: 0.76861191\n","Epoch [42/50], Step [105/164], Loss: 0.61887807\n","Epoch [42/50], Step [120/164], Loss: 0.62972754\n","Epoch [42/50], Step [135/164], Loss: 0.73157400\n","Epoch [42/50], Step [150/164], Loss: 0.67163789\n","Evaluating model in epoch 42 Val loss/acc: 0.97361062|73.4722 Test loss/acc: 1.01645771|72.1861\n","Epoch [43/50], Step [15/164], Loss: 0.61131853\n","Epoch [43/50], Step [30/164], Loss: 0.71359324\n","Epoch [43/50], Step [45/164], Loss: 0.76461792\n","Epoch [43/50], Step [60/164], Loss: 0.66819561\n","Epoch [43/50], Step [75/164], Loss: 0.69610471\n","Epoch [43/50], Step [90/164], Loss: 0.62938762\n","Epoch [43/50], Step [105/164], Loss: 0.68055713\n","Epoch [43/50], Step [120/164], Loss: 0.76130158\n","Epoch [43/50], Step [135/164], Loss: 0.71373862\n","Epoch [43/50], Step [150/164], Loss: 0.52204436\n","Evaluating model in epoch 43 Val loss/acc: 0.97524696|73.6302 Test loss/acc: 1.01700392|72.3862\n","Epoch [44/50], Step [15/164], Loss: 0.66546267\n","Epoch [44/50], Step [30/164], Loss: 0.57588184\n","Epoch [44/50], Step [45/164], Loss: 0.68675405\n","Epoch [44/50], Step [60/164], Loss: 0.70492995\n","Epoch [44/50], Step [75/164], Loss: 0.64315039\n","Epoch [44/50], Step [90/164], Loss: 0.77205479\n","Epoch [44/50], Step [105/164], Loss: 0.57153773\n","Epoch [44/50], Step [120/164], Loss: 0.66227728\n","Epoch [44/50], Step [135/164], Loss: 0.61734539\n","Epoch [44/50], Step [150/164], Loss: 0.63603109\n","Evaluating model in epoch 44 Val loss/acc: 0.97393148|73.7585 Test loss/acc: 1.01427495|72.7864\n","Epoch [45/50], Step [15/164], Loss: 0.66785824\n","Epoch [45/50], Step [30/164], Loss: 0.66195482\n","Epoch [45/50], Step [45/164], Loss: 0.55841219\n","Epoch [45/50], Step [60/164], Loss: 0.66223890\n","Epoch [45/50], Step [75/164], Loss: 0.73544729\n","Epoch [45/50], Step [90/164], Loss: 0.72604346\n","Epoch [45/50], Step [105/164], Loss: 0.58849549\n","Epoch [45/50], Step [120/164], Loss: 0.66901100\n","Epoch [45/50], Step [135/164], Loss: 0.61639607\n","Epoch [45/50], Step [150/164], Loss: 0.61855191\n","Evaluating model in epoch 45 Val loss/acc: 0.98364774|73.5413 Test loss/acc: 1.03024179|72.6863\n","Epoch [46/50], Step [15/164], Loss: 0.56617427\n","Epoch [46/50], Step [30/164], Loss: 0.71322250\n","Epoch [46/50], Step [45/164], Loss: 0.57313967\n","Epoch [46/50], Step [60/164], Loss: 0.71294397\n","Epoch [46/50], Step [75/164], Loss: 0.59876961\n","Epoch [46/50], Step [90/164], Loss: 0.60452545\n","Epoch [46/50], Step [105/164], Loss: 0.69289482\n","Epoch [46/50], Step [120/164], Loss: 0.75118494\n","Epoch [46/50], Step [135/164], Loss: 0.70380187\n","Epoch [46/50], Step [150/164], Loss: 0.66462666\n","Evaluating model in epoch 46 Val loss/acc: 0.98386526|73.8375 Test loss/acc: 1.02463736|72.1361\n","Epoch [47/50], Step [15/164], Loss: 0.75703949\n","Epoch [47/50], Step [30/164], Loss: 0.64408785\n","Epoch [47/50], Step [45/164], Loss: 0.75933951\n","Epoch [47/50], Step [60/164], Loss: 0.57161331\n","Epoch [47/50], Step [75/164], Loss: 0.72473997\n","Epoch [47/50], Step [90/164], Loss: 0.60376596\n","Epoch [47/50], Step [105/164], Loss: 0.61315048\n","Epoch [47/50], Step [120/164], Loss: 0.73394096\n","Epoch [47/50], Step [135/164], Loss: 0.69590431\n","Epoch [47/50], Step [150/164], Loss: 0.67654002\n","Evaluating model in epoch 47 Val loss/acc: 0.97772274|73.5611 Test loss/acc: 1.02096808|72.8364\n","Epoch [48/50], Step [15/164], Loss: 0.60798723\n","Epoch [48/50], Step [30/164], Loss: 0.63118643\n","Epoch [48/50], Step [45/164], Loss: 0.66959357\n","Epoch [48/50], Step [60/164], Loss: 0.57378471\n","Epoch [48/50], Step [75/164], Loss: 0.77396291\n","Epoch [48/50], Step [90/164], Loss: 0.63601351\n","Epoch [48/50], Step [105/164], Loss: 0.60766530\n","Epoch [48/50], Step [120/164], Loss: 0.66669273\n","Epoch [48/50], Step [135/164], Loss: 0.59544903\n","Epoch [48/50], Step [150/164], Loss: 0.64238882\n","Evaluating model in epoch 48 Val loss/acc: 0.98533876|73.8572 Test loss/acc: 1.03525174|72.8364\n","Epoch [49/50], Step [15/164], Loss: 0.52165258\n","Epoch [49/50], Step [30/164], Loss: 0.61002511\n","Epoch [49/50], Step [45/164], Loss: 0.87362623\n","Epoch [49/50], Step [60/164], Loss: 0.57920915\n","Epoch [49/50], Step [75/164], Loss: 0.55633247\n","Epoch [49/50], Step [90/164], Loss: 0.63204634\n","Epoch [49/50], Step [105/164], Loss: 0.50011241\n","Epoch [49/50], Step [120/164], Loss: 0.76871502\n","Epoch [49/50], Step [135/164], Loss: 0.57260674\n","Epoch [49/50], Step [150/164], Loss: 0.71972966\n","Evaluating model in epoch 49 Val loss/acc: 0.98605682|73.5018 Test loss/acc: 1.03536460|72.2361\n","Epoch [50/50], Step [15/164], Loss: 0.59080809\n","Epoch [50/50], Step [30/164], Loss: 0.54885733\n","Epoch [50/50], Step [45/164], Loss: 0.70458907\n","Epoch [50/50], Step [60/164], Loss: 0.53397202\n","Epoch [50/50], Step [75/164], Loss: 0.77852315\n","Epoch [50/50], Step [90/164], Loss: 0.70719612\n","Epoch [50/50], Step [105/164], Loss: 0.43327382\n","Epoch [50/50], Step [120/164], Loss: 0.71360797\n","Epoch [50/50], Step [135/164], Loss: 0.74056053\n","Epoch [50/50], Step [150/164], Loss: 0.50962508\n","Evaluating model in epoch 50 Val loss/acc: 0.98381938|73.6104 Test loss/acc: 1.02548340|72.5363\n","Test loss/accuracy: 1.0255/72.5363\n"]}]},{"cell_type":"markdown","metadata":{"id":"rJYvTeS_iiyG"},"source":["# Ploting at the learning curves"]},{"cell_type":"code","metadata":{"id":"ZQFFMz8EimvM"},"source":["%load_ext tensorboard\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]}]}